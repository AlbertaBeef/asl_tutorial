{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2321d8cb",
   "metadata": {
    "id": "2321d8cb"
   },
   "source": [
    "<h1 style=\"font-size:30px;\">Deploying the ASL Classifier to Vitis-AI</h1>  \n",
    "\n",
    "This notebook describes how to quantize and compile a TensorFlow2 model with Vitis-AI for deployment.\n",
    "\n",
    "<img src='./images/VGG16_06_asl_fine_tuning.png' width=1000 align='center'><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f452f7",
   "metadata": {
    "id": "f5f452f7"
   },
   "source": [
    "## Table of Contents\n",
    "* [1 System Configuration](#1-System-Configuration)\n",
    "* [2 Download and Extract the Dataset](#2-Download-and-Extract-the-Dataset)\n",
    "* [3 Dataset Configuration](#3-Dataset-Configuration)\n",
    "* [4 Quantization](#4-Quantization)\n",
    "* [5 Compilation](#5-Compilation)\n",
    "* [6 Conclusion](#6-Conclusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d103ad0",
   "metadata": {
    "id": "0d103ad0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 20:43:05.782217: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-26 20:43:05.912263: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import zipfile\n",
    "import requests\n",
    "import glob as glob\n",
    "\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter)\n",
    "from dataclasses import dataclass \n",
    "\n",
    "block_plot = False\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "SEED_VALUE = 42 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42883a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version :  2.10.0\n",
      "tensorflow version :  2.10.0\n",
      "opencv version :  4.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow version : \",tf.__version__)\n",
    "print(\"tensorflow version : \",keras.__version__)\n",
    "print(\"opencv version : \",cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61afbd",
   "metadata": {
    "id": "bc61afbd"
   },
   "source": [
    "## 1 System Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450b9a9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "450b9a9a",
    "outputId": "4b0ff0e3-fc7d-4689-a8f4-953b7b7fa392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "def system_config():\n",
    "    \n",
    "    # Get list of GPUs.\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    print(gpu_devices)\n",
    "    \n",
    "    if len(gpu_devices) > 0:\n",
    "        print('Using GPU')\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "        os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n",
    "        \n",
    "        # If there are any gpu devices, use first gpu.\n",
    "        tf.config.experimental.set_visible_devices(gpu_devices[0], 'GPU')\n",
    "        \n",
    "        # Grow the memory usage as it is needed by the process.\n",
    "        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "        \n",
    "        # Enable using cudNN.\n",
    "        os.environ['TF_USE_CUDNN'] = \"true\"\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "\n",
    "system_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfe4cd",
   "metadata": {
    "id": "97cfe4cd"
   },
   "source": [
    "## 2 Download and Extract the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c6b79b",
   "metadata": {
    "id": "03c6b79b"
   },
   "outputs": [],
   "source": [
    "def download_file(url, save_name):\n",
    "    url = url\n",
    "    file = requests.get(url)\n",
    "\n",
    "    open(save_name, 'wb').write(file.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf238d22",
   "metadata": {
    "id": "bf238d22"
   },
   "outputs": [],
   "source": [
    "def unzip(zip_file=None):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file) as z:\n",
    "            z.extractall(\"./\")\n",
    "            print(\"Extracted all\")\n",
    "    except:\n",
    "        print(\"Invalid file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55f0cb6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55f0cb6a",
    "outputId": "0e54ae7b-cb38-4469-9848-baa3addb5d47"
   },
   "outputs": [],
   "source": [
    "#download_file(\n",
    "#    'https://github.com/AlbertaBeef/asl_tutorial/releases/download/vitis_ai_3.0_version2/dataset_ASL_reduced.zip?dl=1', \n",
    "#    'dataset_ASL_reduced.zip'\n",
    "#)\n",
    "    \n",
    "#unzip(zip_file='dataset_ASL_reduced.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8158104",
   "metadata": {
    "id": "d8158104"
   },
   "source": [
    "## 3 Dataset and Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b8e05f",
   "metadata": {
    "id": "a9b8e05f"
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    NUM_CLASSES: int = 29\n",
    "    IMG_HEIGHT:  int = 224\n",
    "    IMG_WIDTH:   int = 224\n",
    "    CHANNELS:    int = 3\n",
    "    BATCH_SIZE:  int = 32\n",
    "    TRAINING_DATA_ROOT:   str = './dataset_ASL_reduced/training'\n",
    "    VALIDATION_DATA_ROOT:   str = './dataset_ASL_reduced/validation'\n",
    "        \n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE:     int   = 32\n",
    "    EPOCHS:         int   = 51\n",
    "    LEARNING_RATE:  float = 0.0001\n",
    "    CHECKPOINT_DIR: str   = './saved_models_asl_classifier3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddde6491",
   "metadata": {
    "id": "ddde6491"
   },
   "source": [
    "### 3.1 Prepare the Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "927f1028",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "927f1028",
    "outputId": "85c82008-4818-45a3-819d-0622b7c8db1a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5800 files belonging to 29 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 20:43:08.692487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1450 files belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(directory=DatasetConfig.TRAINING_DATA_ROOT,\n",
    "                                             batch_size=TrainingConfig.BATCH_SIZE,\n",
    "                                             shuffle=True,\n",
    "                                             seed=SEED_VALUE,\n",
    "                                             label_mode='categorical',\n",
    "                                             image_size=(DatasetConfig.IMG_WIDTH, DatasetConfig.IMG_HEIGHT),\n",
    "                                            )\n",
    "\n",
    "valid_dataset = image_dataset_from_directory(directory=DatasetConfig.VALIDATION_DATA_ROOT,\n",
    "                                             batch_size=TrainingConfig.BATCH_SIZE,\n",
    "                                             shuffle=True,\n",
    "                                             seed=SEED_VALUE,\n",
    "                                             label_mode='categorical',\n",
    "                                             image_size=(DatasetConfig.IMG_WIDTH, DatasetConfig.IMG_HEIGHT),\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960fa33c",
   "metadata": {
    "id": "960fa33c"
   },
   "source": [
    "## 4 Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042806bf",
   "metadata": {},
   "source": [
    "**Load model**\n",
    "\n",
    "Load model for the rest of the tutorial with the `load_model` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f10bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.load_model('tf2_asl_classifier13.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17052db7",
   "metadata": {},
   "source": [
    "In order to compile the trained model for deployment on a DPU platform, we must first quantize it. Here we will use the `vitis_quantize` module to convert the floating point model into an INT8 quantized representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aab4b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.quantization.keras import vitis_quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0c299",
   "metadata": {},
   "source": [
    "**Quantize model**\n",
    "\n",
    "By default the `quantize_model` function converts the weights, activations and inputs into 8-bit wide numbers. We can specify different values and configurations using `weight_bit`, `activation_bit` and other parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3ec24d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI INFO] Update activation_bit: 8\n",
      "[VAI INFO] Update weight_bit: 8\n",
      "[VAI INFO] Quantizing without specific `target`.\n",
      "[VAI INFO] Start CrossLayerEqualization...\n",
      "10/10 [==============================] - 5s 527ms/step\n",
      "[VAI INFO] CrossLayerEqualization Done.\n",
      "[VAI INFO] Start Quantize Calibration...\n",
      "46/46 [==============================] - 268s 6s/step\n",
      "[VAI INFO] Quantize Calibration Done.\n",
      "[VAI INFO] Start Post-Quant Model Refinement...\n",
      "[VAI INFO] Start Quantize Position Ajustment...\n",
      "[VAI INFO] Quantize Position Ajustment Done.\n",
      "[VAI INFO] Post-Quant Model Refninement Done.\n",
      "[VAI INFO] Start Model Finalization...\n",
      "[VAI INFO] Model Finalization Done.\n",
      "[VAI INFO] Quantization Finished.\n"
     ]
    }
   ],
   "source": [
    "quantizer = vitis_quantize.VitisQuantizer(model)\n",
    "quantized_model = quantizer.quantize_model(calib_dataset=valid_dataset, weight_bit=8, activation_bit=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0236c8a",
   "metadata": {},
   "source": [
    "**Evaluate quantized model**\n",
    "\n",
    "In order to evaluate the quantized model, it needs to be re-compiled with the desired loss and evaluation metrics, such as accuracy. Since we are using 8-bit quantization we do not lose much performance, if at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26efd672",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.compile(loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a02f6475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182/182 [==============================] - 459s 3s/step - loss: 1.4657e-06 - accuracy: 1.0000\n",
      "Model evaluation accuracy (training dataset): 100.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model evaluation accuracy (training dataset): {quantized_model.evaluate(train_dataset)[1]*100.:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05fb1fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 108s 2s/step - loss: 0.0581 - accuracy: 0.9931\n",
      "Model evaluation accuracy: 99.310\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model evaluation accuracy (validation dataset): {quantized_model.evaluate(valid_dataset)[1]*100.:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb228f43",
   "metadata": {},
   "source": [
    "**Save quantized model**\n",
    "\n",
    "Once we are happy with the performance of the quantized model, we can save it as a .h5 file, simply using the `save` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eb4a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.save('tf2_asl_classifier13_quantized.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58451641",
   "metadata": {},
   "source": [
    "## 5 Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974c603",
   "metadata": {},
   "source": [
    "For this final step we use the Vitis AI compiler `vai_c_tensorflow2` and pass the quantized model as a parameter. \n",
    "\n",
    "The target platform (ie. specific DPU architecture) is defined by .arch file.\n",
    "\n",
    "To support as many platforms as possible, we compile for the following DPU architectures:\n",
    "- B4096 (ZCU102, ZCU104, UltraZed-EV)\n",
    "- B3136 (KV260)\n",
    "- B2304 (Ultra96-V2)\n",
    "- B1152 (Ultra96-V2+DualCam)\n",
    "-  B512 (ZUBoard)\n",
    "-  B128 (ZUBoard+DualCam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ecf09d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_DPUCZDX8G_ISA1_B4096_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 24867.42it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 884.61it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 181.99it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 760.25it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 255.47it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_DPUCZDX8G_ISA1_B4096_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B4096\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B4096/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B4096//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is bfad8a3965ccd37cf23d06f698057b85, and has been saved to \"/workspace/./model/B4096/md5sum.txt\"\n",
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_DPUCZDX8G_ISA1_B3136_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 24792.04it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 886.22it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 186.88it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 759.29it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 253.74it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_DPUCZDX8G_ISA1_B3136_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B3136\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B3136/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B3136//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is f556cc80f77a2752929e8663ee813650, and has been saved to \"/workspace/./model/B3136/md5sum.txt\"\n",
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_0x101000016010405_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 25259.09it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 906.18it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 189.69it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 753.49it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 262.56it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_0x101000016010405_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B2304/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B2304//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 541b56fabdf5afa302795d42890f6a74, and has been saved to \"/workspace/./model/B2304/md5sum.txt\"\n",
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_0x101000017010203_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 25611.06it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 888.10it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 191.24it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 773.64it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 259.80it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_0x101000017010203_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B1152_0101000017010203\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B1152/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B1152//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 306c3dec6459ab4151b49522b2a7b4fe, and has been saved to \"/workspace/./model/B1152/md5sum.txt\"\n",
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_0x101000016010200_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 25451.67it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 890.96it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 187.28it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 737.59it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 258.64it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_0x101000016010200_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B512_0101000016010200\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B512/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B512//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 12a3ae032c324d2cb00c2ba21c5ab3be, and has been saved to \"/workspace/./model/B512/md5sum.txt\"\n",
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_0x101000002010208_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 24848.53it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 886.08it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 190.39it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 761.77it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 259.20it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_0x101000002010208_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B128_0101000002010208\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B128/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B128//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is a39def16c7bc8413de3b67f5f808449f, and has been saved to \"/workspace/./model/B128/md5sum.txt\"\n"
     ]
    }
   ],
   "source": [
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier13_quantized.h5 \\\n",
    "    --arch ./arch/B4096/arch-zcu104.json \\\n",
    "    --output_dir ./model_vgg16/B4096/ \\\n",
    "    --net_name asl_classifier\n",
    "\n",
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier13_quantized.h5 \\\n",
    "    --arch ./arch/B3136/arch-kv260.json \\\n",
    "    --output_dir ./model_vgg16/B3136/ \\\n",
    "    --net_name asl_classifier\n",
    "\n",
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier13_quantized.h5 \\\n",
    "    --arch ./arch/B2304/arch-b2304-lr.json \\\n",
    "    --output_dir ./model_vgg16/B2304/ \\\n",
    "    --net_name asl_classifier\n",
    "\n",
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier13_quantized.h5 \\\n",
    "    --arch ./arch/B1152/arch-b1152-hr.json \\\n",
    "    --output_dir ./model_vgg16/B1152/ \\\n",
    "    --net_name asl_classifier\n",
    "\n",
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier13_quantized.h5 \\\n",
    "    --arch ./arch/B512/arch-b512-lr.json \\\n",
    "    --output_dir ./model_vgg16/B512/ \\\n",
    "    --net_name asl_classifier\n",
    "\n",
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier13_quantized.h5 \\\n",
    "    --arch ./arch/B128/arch-b128-lr.json \\\n",
    "    --output_dir ./model_vgg16/B128/ \\\n",
    "    --net_name asl_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d4699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4954c68",
   "metadata": {},
   "source": [
    "### Generate test-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87e332c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './test-images'\n",
    "\n",
    "   \n",
    "def generate_test_images(dataset, checkpoint_dir=None, checkpoint_version=0):\n",
    "    \n",
    "    if not checkpoint_dir:\n",
    "        checkpoint_dir = os.path.join(os.getcwd(), TrainingConfig.checkpoint_dir, f\"version_{checkpoint_version}\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "         \n",
    "    # Load saved model.\n",
    "    model = tf.keras.models.load_model(checkpoint_dir)\n",
    "    \n",
    "    num_test_images = 850\n",
    "    class_names = dataset.class_names\n",
    "    jdx = 0\n",
    "    \n",
    "    # Evaluate all the batches.\n",
    "    for image_batch, labels_batch in dataset:\n",
    "        \n",
    "        # Predictions for the current batch.\n",
    "        predictions = model.predict(image_batch)\n",
    "        \n",
    "        # Loop over all the images in the current batch.\n",
    "        for idx in range(len(labels_batch)):\n",
    "            \n",
    "            pred_idx = tf.argmax(predictions[idx]).numpy()\n",
    "            truth_idx = np.nonzero(labels_batch[idx].numpy())\n",
    "            \n",
    "            # Save the images with correct predictions\n",
    "            if pred_idx == truth_idx:\n",
    "                \n",
    "                jdx += 1\n",
    "                \n",
    "                if jdx > num_test_images:\n",
    "                    # Break from the loops if the maximum number of images have been plotted\n",
    "                    break\n",
    "                \n",
    "                image = image_batch[idx].numpy().astype(\"uint8\")\n",
    "                image_dst = output_dir+\"/test%04d\"%(jdx)+'_'+str(pred_idx)+'_'+str(class_names[pred_idx])+'.png'\n",
    "                if not os.path.exists(image_dst):\n",
    "                    print(image_dst)\n",
    "                    cv2.imwrite(image_dst, image )\n",
    "            \n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "231a65e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0001_16_Q.png\n",
      "./test-images/test0002_14_O.png\n",
      "./test-images/test0003_7_H.png\n",
      "./test-images/test0004_27_nothing.png\n",
      "./test-images/test0005_12_M.png\n",
      "./test-images/test0006_17_R.png\n",
      "./test-images/test0007_3_D.png\n",
      "./test-images/test0008_0_A.png\n",
      "./test-images/test0009_21_V.png\n",
      "./test-images/test0010_19_T.png\n",
      "./test-images/test0011_18_S.png\n",
      "./test-images/test0012_11_L.png\n",
      "./test-images/test0013_4_E.png\n",
      "./test-images/test0014_20_U.png\n",
      "./test-images/test0015_28_space.png\n",
      "./test-images/test0016_20_U.png\n",
      "./test-images/test0017_20_U.png\n",
      "./test-images/test0018_8_I.png\n",
      "./test-images/test0019_0_A.png\n",
      "./test-images/test0020_16_Q.png\n",
      "./test-images/test0021_21_V.png\n",
      "./test-images/test0022_15_P.png\n",
      "./test-images/test0023_17_R.png\n",
      "./test-images/test0024_18_S.png\n",
      "./test-images/test0025_11_L.png\n",
      "./test-images/test0026_9_J.png\n",
      "./test-images/test0027_17_R.png\n",
      "./test-images/test0028_27_nothing.png\n",
      "./test-images/test0029_1_B.png\n",
      "./test-images/test0030_15_P.png\n",
      "./test-images/test0031_19_T.png\n",
      "./test-images/test0032_8_I.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0033_11_L.png\n",
      "./test-images/test0034_19_T.png\n",
      "./test-images/test0035_1_B.png\n",
      "./test-images/test0036_28_space.png\n",
      "./test-images/test0037_2_C.png\n",
      "./test-images/test0038_26_del.png\n",
      "./test-images/test0039_10_K.png\n",
      "./test-images/test0040_9_J.png\n",
      "./test-images/test0041_15_P.png\n",
      "./test-images/test0042_2_C.png\n",
      "./test-images/test0043_15_P.png\n",
      "./test-images/test0044_18_S.png\n",
      "./test-images/test0045_12_M.png\n",
      "./test-images/test0046_28_space.png\n",
      "./test-images/test0047_12_M.png\n",
      "./test-images/test0048_12_M.png\n",
      "./test-images/test0049_19_T.png\n",
      "./test-images/test0050_6_G.png\n",
      "./test-images/test0051_4_E.png\n",
      "./test-images/test0052_1_B.png\n",
      "./test-images/test0053_10_K.png\n",
      "./test-images/test0054_4_E.png\n",
      "./test-images/test0055_13_N.png\n",
      "./test-images/test0056_14_O.png\n",
      "./test-images/test0057_19_T.png\n",
      "./test-images/test0058_5_F.png\n",
      "./test-images/test0059_13_N.png\n",
      "./test-images/test0060_22_W.png\n",
      "./test-images/test0061_11_L.png\n",
      "./test-images/test0062_7_H.png\n",
      "./test-images/test0063_21_V.png\n",
      "./test-images/test0064_25_Z.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0065_9_J.png\n",
      "./test-images/test0066_0_A.png\n",
      "./test-images/test0067_22_W.png\n",
      "./test-images/test0068_10_K.png\n",
      "./test-images/test0069_5_F.png\n",
      "./test-images/test0070_4_E.png\n",
      "./test-images/test0071_13_N.png\n",
      "./test-images/test0072_6_G.png\n",
      "./test-images/test0073_23_X.png\n",
      "./test-images/test0074_27_nothing.png\n",
      "./test-images/test0075_5_F.png\n",
      "./test-images/test0076_8_I.png\n",
      "./test-images/test0077_18_S.png\n",
      "./test-images/test0078_8_I.png\n",
      "./test-images/test0079_8_I.png\n",
      "./test-images/test0080_11_L.png\n",
      "./test-images/test0081_24_Y.png\n",
      "./test-images/test0082_9_J.png\n",
      "./test-images/test0083_0_A.png\n",
      "./test-images/test0084_14_O.png\n",
      "./test-images/test0085_15_P.png\n",
      "./test-images/test0086_28_space.png\n",
      "./test-images/test0087_7_H.png\n",
      "./test-images/test0088_8_I.png\n",
      "./test-images/test0089_17_R.png\n",
      "./test-images/test0090_22_W.png\n",
      "./test-images/test0091_3_D.png\n",
      "./test-images/test0092_1_B.png\n",
      "./test-images/test0093_21_V.png\n",
      "./test-images/test0094_14_O.png\n",
      "./test-images/test0095_22_W.png\n",
      "./test-images/test0096_15_P.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0097_5_F.png\n",
      "./test-images/test0098_20_U.png\n",
      "./test-images/test0099_2_C.png\n",
      "./test-images/test0100_11_L.png\n",
      "./test-images/test0101_4_E.png\n",
      "./test-images/test0102_26_del.png\n",
      "./test-images/test0103_5_F.png\n",
      "./test-images/test0104_8_I.png\n",
      "./test-images/test0105_17_R.png\n",
      "./test-images/test0106_2_C.png\n",
      "./test-images/test0107_0_A.png\n",
      "./test-images/test0108_26_del.png\n",
      "./test-images/test0109_25_Z.png\n",
      "./test-images/test0110_15_P.png\n",
      "./test-images/test0111_8_I.png\n",
      "./test-images/test0112_5_F.png\n",
      "./test-images/test0113_6_G.png\n",
      "./test-images/test0114_5_F.png\n",
      "./test-images/test0115_12_M.png\n",
      "./test-images/test0116_12_M.png\n",
      "./test-images/test0117_15_P.png\n",
      "./test-images/test0118_23_X.png\n",
      "./test-images/test0119_18_S.png\n",
      "./test-images/test0120_10_K.png\n",
      "./test-images/test0121_22_W.png\n",
      "./test-images/test0122_2_C.png\n",
      "./test-images/test0123_18_S.png\n",
      "./test-images/test0124_4_E.png\n",
      "./test-images/test0125_1_B.png\n",
      "./test-images/test0126_23_X.png\n",
      "./test-images/test0127_10_K.png\n",
      "./test-images/test0128_22_W.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0129_16_Q.png\n",
      "./test-images/test0130_4_E.png\n",
      "./test-images/test0131_2_C.png\n",
      "./test-images/test0132_23_X.png\n",
      "./test-images/test0133_10_K.png\n",
      "./test-images/test0134_17_R.png\n",
      "./test-images/test0135_5_F.png\n",
      "./test-images/test0136_28_space.png\n",
      "./test-images/test0137_27_nothing.png\n",
      "./test-images/test0138_1_B.png\n",
      "./test-images/test0139_9_J.png\n",
      "./test-images/test0140_28_space.png\n",
      "./test-images/test0141_2_C.png\n",
      "./test-images/test0142_20_U.png\n",
      "./test-images/test0143_12_M.png\n",
      "./test-images/test0144_25_Z.png\n",
      "./test-images/test0145_24_Y.png\n",
      "./test-images/test0146_18_S.png\n",
      "./test-images/test0147_21_V.png\n",
      "./test-images/test0148_25_Z.png\n",
      "./test-images/test0149_13_N.png\n",
      "./test-images/test0150_1_B.png\n",
      "./test-images/test0151_22_W.png\n",
      "./test-images/test0152_3_D.png\n",
      "./test-images/test0153_2_C.png\n",
      "./test-images/test0154_3_D.png\n",
      "./test-images/test0155_17_R.png\n",
      "./test-images/test0156_13_N.png\n",
      "./test-images/test0157_18_S.png\n",
      "./test-images/test0158_10_K.png\n",
      "./test-images/test0159_3_D.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0160_13_N.png\n",
      "./test-images/test0161_2_C.png\n",
      "./test-images/test0162_20_U.png\n",
      "./test-images/test0163_26_del.png\n",
      "./test-images/test0164_12_M.png\n",
      "./test-images/test0165_27_nothing.png\n",
      "./test-images/test0166_20_U.png\n",
      "./test-images/test0167_20_U.png\n",
      "./test-images/test0168_1_B.png\n",
      "./test-images/test0169_27_nothing.png\n",
      "./test-images/test0170_20_U.png\n",
      "./test-images/test0171_23_X.png\n",
      "./test-images/test0172_5_F.png\n",
      "./test-images/test0173_25_Z.png\n",
      "./test-images/test0174_17_R.png\n",
      "./test-images/test0175_18_S.png\n",
      "./test-images/test0176_12_M.png\n",
      "./test-images/test0177_7_H.png\n",
      "./test-images/test0178_2_C.png\n",
      "./test-images/test0179_26_del.png\n",
      "./test-images/test0180_7_H.png\n",
      "./test-images/test0181_7_H.png\n",
      "./test-images/test0182_9_J.png\n",
      "./test-images/test0183_11_L.png\n",
      "./test-images/test0184_11_L.png\n",
      "./test-images/test0185_23_X.png\n",
      "./test-images/test0186_7_H.png\n",
      "./test-images/test0187_17_R.png\n",
      "./test-images/test0188_8_I.png\n",
      "./test-images/test0189_18_S.png\n",
      "./test-images/test0190_7_H.png\n",
      "./test-images/test0191_9_J.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0192_11_L.png\n",
      "./test-images/test0193_20_U.png\n",
      "./test-images/test0194_2_C.png\n",
      "./test-images/test0195_3_D.png\n",
      "./test-images/test0196_28_space.png\n",
      "./test-images/test0197_4_E.png\n",
      "./test-images/test0198_5_F.png\n",
      "./test-images/test0199_7_H.png\n",
      "./test-images/test0200_21_V.png\n",
      "./test-images/test0201_12_M.png\n",
      "./test-images/test0202_24_Y.png\n",
      "./test-images/test0203_7_H.png\n",
      "./test-images/test0204_27_nothing.png\n",
      "./test-images/test0205_27_nothing.png\n",
      "./test-images/test0206_24_Y.png\n",
      "./test-images/test0207_20_U.png\n",
      "./test-images/test0208_27_nothing.png\n",
      "./test-images/test0209_3_D.png\n",
      "./test-images/test0210_4_E.png\n",
      "./test-images/test0211_8_I.png\n",
      "./test-images/test0212_26_del.png\n",
      "./test-images/test0213_11_L.png\n",
      "./test-images/test0214_6_G.png\n",
      "./test-images/test0215_10_K.png\n",
      "./test-images/test0216_12_M.png\n",
      "./test-images/test0217_18_S.png\n",
      "./test-images/test0218_6_G.png\n",
      "./test-images/test0219_21_V.png\n",
      "./test-images/test0220_15_P.png\n",
      "./test-images/test0221_28_space.png\n",
      "./test-images/test0222_2_C.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0223_16_Q.png\n",
      "./test-images/test0224_26_del.png\n",
      "./test-images/test0225_11_L.png\n",
      "./test-images/test0226_14_O.png\n",
      "./test-images/test0227_16_Q.png\n",
      "./test-images/test0228_7_H.png\n",
      "./test-images/test0229_6_G.png\n",
      "./test-images/test0230_17_R.png\n",
      "./test-images/test0231_9_J.png\n",
      "./test-images/test0232_15_P.png\n",
      "./test-images/test0233_16_Q.png\n",
      "./test-images/test0234_5_F.png\n",
      "./test-images/test0235_18_S.png\n",
      "./test-images/test0236_2_C.png\n",
      "./test-images/test0237_22_W.png\n",
      "./test-images/test0238_20_U.png\n",
      "./test-images/test0239_16_Q.png\n",
      "./test-images/test0240_15_P.png\n",
      "./test-images/test0241_22_W.png\n",
      "./test-images/test0242_25_Z.png\n",
      "./test-images/test0243_23_X.png\n",
      "./test-images/test0244_10_K.png\n",
      "./test-images/test0245_9_J.png\n",
      "./test-images/test0246_8_I.png\n",
      "./test-images/test0247_0_A.png\n",
      "./test-images/test0248_3_D.png\n",
      "./test-images/test0249_23_X.png\n",
      "./test-images/test0250_19_T.png\n",
      "./test-images/test0251_14_O.png\n",
      "./test-images/test0252_11_L.png\n",
      "./test-images/test0253_6_G.png\n",
      "./test-images/test0254_28_space.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0255_26_del.png\n",
      "./test-images/test0256_14_O.png\n",
      "./test-images/test0257_14_O.png\n",
      "./test-images/test0258_15_P.png\n",
      "./test-images/test0259_5_F.png\n",
      "./test-images/test0260_16_Q.png\n",
      "./test-images/test0261_15_P.png\n",
      "./test-images/test0262_19_T.png\n",
      "./test-images/test0263_22_W.png\n",
      "./test-images/test0264_5_F.png\n",
      "./test-images/test0265_21_V.png\n",
      "./test-images/test0266_5_F.png\n",
      "./test-images/test0267_20_U.png\n",
      "./test-images/test0268_1_B.png\n",
      "./test-images/test0269_26_del.png\n",
      "./test-images/test0270_11_L.png\n",
      "./test-images/test0271_2_C.png\n",
      "./test-images/test0272_18_S.png\n",
      "./test-images/test0273_8_I.png\n",
      "./test-images/test0274_3_D.png\n",
      "./test-images/test0275_22_W.png\n",
      "./test-images/test0276_28_space.png\n",
      "./test-images/test0277_0_A.png\n",
      "./test-images/test0278_4_E.png\n",
      "./test-images/test0279_28_space.png\n",
      "./test-images/test0280_18_S.png\n",
      "./test-images/test0281_9_J.png\n",
      "./test-images/test0282_3_D.png\n",
      "./test-images/test0283_17_R.png\n",
      "./test-images/test0284_21_V.png\n",
      "./test-images/test0285_18_S.png\n",
      "./test-images/test0286_7_H.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0287_11_L.png\n",
      "./test-images/test0288_14_O.png\n",
      "./test-images/test0289_22_W.png\n",
      "./test-images/test0290_6_G.png\n",
      "./test-images/test0291_16_Q.png\n",
      "./test-images/test0292_8_I.png\n",
      "./test-images/test0293_6_G.png\n",
      "./test-images/test0294_8_I.png\n",
      "./test-images/test0295_10_K.png\n",
      "./test-images/test0296_12_M.png\n",
      "./test-images/test0297_16_Q.png\n",
      "./test-images/test0298_0_A.png\n",
      "./test-images/test0299_13_N.png\n",
      "./test-images/test0300_27_nothing.png\n",
      "./test-images/test0301_9_J.png\n",
      "./test-images/test0302_5_F.png\n",
      "./test-images/test0303_21_V.png\n",
      "./test-images/test0304_8_I.png\n",
      "./test-images/test0305_26_del.png\n",
      "./test-images/test0306_28_space.png\n",
      "./test-images/test0307_24_Y.png\n",
      "./test-images/test0308_28_space.png\n",
      "./test-images/test0309_10_K.png\n",
      "./test-images/test0310_10_K.png\n",
      "./test-images/test0311_1_B.png\n",
      "./test-images/test0312_24_Y.png\n",
      "./test-images/test0313_22_W.png\n",
      "./test-images/test0314_17_R.png\n",
      "./test-images/test0315_0_A.png\n",
      "./test-images/test0316_3_D.png\n",
      "./test-images/test0317_15_P.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0318_7_H.png\n",
      "./test-images/test0319_19_T.png\n",
      "./test-images/test0320_4_E.png\n",
      "./test-images/test0321_7_H.png\n",
      "./test-images/test0322_20_U.png\n",
      "./test-images/test0323_6_G.png\n",
      "./test-images/test0324_15_P.png\n",
      "./test-images/test0325_28_space.png\n",
      "./test-images/test0326_23_X.png\n",
      "./test-images/test0327_26_del.png\n",
      "./test-images/test0328_13_N.png\n",
      "./test-images/test0329_24_Y.png\n",
      "./test-images/test0330_11_L.png\n",
      "./test-images/test0331_2_C.png\n",
      "./test-images/test0332_9_J.png\n",
      "./test-images/test0333_19_T.png\n",
      "./test-images/test0334_23_X.png\n",
      "./test-images/test0335_23_X.png\n",
      "./test-images/test0336_18_S.png\n",
      "./test-images/test0337_11_L.png\n",
      "./test-images/test0338_7_H.png\n",
      "./test-images/test0339_21_V.png\n",
      "./test-images/test0340_3_D.png\n",
      "./test-images/test0341_9_J.png\n",
      "./test-images/test0342_16_Q.png\n",
      "./test-images/test0343_23_X.png\n",
      "./test-images/test0344_9_J.png\n",
      "./test-images/test0345_21_V.png\n",
      "./test-images/test0346_6_G.png\n",
      "./test-images/test0347_4_E.png\n",
      "./test-images/test0348_15_P.png\n",
      "./test-images/test0349_23_X.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0350_26_del.png\n",
      "./test-images/test0351_23_X.png\n",
      "./test-images/test0352_2_C.png\n",
      "./test-images/test0353_4_E.png\n",
      "./test-images/test0354_6_G.png\n",
      "./test-images/test0355_22_W.png\n",
      "./test-images/test0356_2_C.png\n",
      "./test-images/test0357_18_S.png\n",
      "./test-images/test0358_28_space.png\n",
      "./test-images/test0359_4_E.png\n",
      "./test-images/test0360_0_A.png\n",
      "./test-images/test0361_11_L.png\n",
      "./test-images/test0362_26_del.png\n",
      "./test-images/test0363_18_S.png\n",
      "./test-images/test0364_5_F.png\n",
      "./test-images/test0365_8_I.png\n",
      "./test-images/test0366_1_B.png\n",
      "./test-images/test0367_18_S.png\n",
      "./test-images/test0368_27_nothing.png\n",
      "./test-images/test0369_6_G.png\n",
      "./test-images/test0370_3_D.png\n",
      "./test-images/test0371_26_del.png\n",
      "./test-images/test0372_25_Z.png\n",
      "./test-images/test0373_20_U.png\n",
      "./test-images/test0374_17_R.png\n",
      "./test-images/test0375_17_R.png\n",
      "./test-images/test0376_18_S.png\n",
      "./test-images/test0377_6_G.png\n",
      "./test-images/test0378_5_F.png\n",
      "./test-images/test0379_13_N.png\n",
      "./test-images/test0380_1_B.png\n",
      "./test-images/test0381_20_U.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0382_15_P.png\n",
      "./test-images/test0383_6_G.png\n",
      "./test-images/test0384_7_H.png\n",
      "./test-images/test0385_14_O.png\n",
      "./test-images/test0386_16_Q.png\n",
      "./test-images/test0387_20_U.png\n",
      "./test-images/test0388_6_G.png\n",
      "./test-images/test0389_5_F.png\n",
      "./test-images/test0390_25_Z.png\n",
      "./test-images/test0391_2_C.png\n",
      "./test-images/test0392_11_L.png\n",
      "./test-images/test0393_25_Z.png\n",
      "./test-images/test0394_13_N.png\n",
      "./test-images/test0395_27_nothing.png\n",
      "./test-images/test0396_20_U.png\n",
      "./test-images/test0397_18_S.png\n",
      "./test-images/test0398_27_nothing.png\n",
      "./test-images/test0399_22_W.png\n",
      "./test-images/test0400_27_nothing.png\n",
      "./test-images/test0401_25_Z.png\n",
      "./test-images/test0402_11_L.png\n",
      "./test-images/test0403_26_del.png\n",
      "./test-images/test0404_18_S.png\n",
      "./test-images/test0405_18_S.png\n",
      "./test-images/test0406_24_Y.png\n",
      "./test-images/test0407_21_V.png\n",
      "./test-images/test0408_1_B.png\n",
      "./test-images/test0409_27_nothing.png\n",
      "./test-images/test0410_6_G.png\n",
      "./test-images/test0411_15_P.png\n",
      "./test-images/test0412_16_Q.png\n",
      "./test-images/test0413_18_S.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0414_7_H.png\n",
      "./test-images/test0415_19_T.png\n",
      "./test-images/test0416_0_A.png\n",
      "./test-images/test0417_10_K.png\n",
      "./test-images/test0418_0_A.png\n",
      "./test-images/test0419_23_X.png\n",
      "./test-images/test0420_1_B.png\n",
      "./test-images/test0421_14_O.png\n",
      "./test-images/test0422_13_N.png\n",
      "./test-images/test0423_22_W.png\n",
      "./test-images/test0424_0_A.png\n",
      "./test-images/test0425_8_I.png\n",
      "./test-images/test0426_7_H.png\n",
      "./test-images/test0427_25_Z.png\n",
      "./test-images/test0428_8_I.png\n",
      "./test-images/test0429_24_Y.png\n",
      "./test-images/test0430_15_P.png\n",
      "./test-images/test0431_1_B.png\n",
      "./test-images/test0432_21_V.png\n",
      "./test-images/test0433_24_Y.png\n",
      "./test-images/test0434_15_P.png\n",
      "./test-images/test0435_4_E.png\n",
      "./test-images/test0436_21_V.png\n",
      "./test-images/test0437_13_N.png\n",
      "./test-images/test0438_2_C.png\n",
      "./test-images/test0439_11_L.png\n",
      "./test-images/test0440_23_X.png\n",
      "./test-images/test0441_21_V.png\n",
      "./test-images/test0442_14_O.png\n",
      "./test-images/test0443_14_O.png\n",
      "./test-images/test0444_10_K.png\n",
      "./test-images/test0445_18_S.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0446_5_F.png\n",
      "./test-images/test0447_16_Q.png\n",
      "./test-images/test0448_12_M.png\n",
      "./test-images/test0449_13_N.png\n",
      "./test-images/test0450_1_B.png\n",
      "./test-images/test0451_28_space.png\n",
      "./test-images/test0452_26_del.png\n",
      "./test-images/test0453_14_O.png\n",
      "./test-images/test0454_6_G.png\n",
      "./test-images/test0455_26_del.png\n",
      "./test-images/test0456_9_J.png\n",
      "./test-images/test0457_22_W.png\n",
      "./test-images/test0458_11_L.png\n",
      "./test-images/test0459_10_K.png\n",
      "./test-images/test0460_23_X.png\n",
      "./test-images/test0461_17_R.png\n",
      "./test-images/test0462_23_X.png\n",
      "./test-images/test0463_16_Q.png\n",
      "./test-images/test0464_8_I.png\n",
      "./test-images/test0465_5_F.png\n",
      "./test-images/test0466_18_S.png\n",
      "./test-images/test0467_16_Q.png\n",
      "./test-images/test0468_10_K.png\n",
      "./test-images/test0469_3_D.png\n",
      "./test-images/test0470_19_T.png\n",
      "./test-images/test0471_5_F.png\n",
      "./test-images/test0472_22_W.png\n",
      "./test-images/test0473_24_Y.png\n",
      "./test-images/test0474_6_G.png\n",
      "./test-images/test0475_8_I.png\n",
      "./test-images/test0476_14_O.png\n",
      "./test-images/test0477_13_N.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0478_19_T.png\n",
      "./test-images/test0479_2_C.png\n",
      "./test-images/test0480_19_T.png\n",
      "./test-images/test0481_1_B.png\n",
      "./test-images/test0482_12_M.png\n",
      "./test-images/test0483_23_X.png\n",
      "./test-images/test0484_20_U.png\n",
      "./test-images/test0485_17_R.png\n",
      "./test-images/test0486_14_O.png\n",
      "./test-images/test0487_7_H.png\n",
      "./test-images/test0488_22_W.png\n",
      "./test-images/test0489_16_Q.png\n",
      "./test-images/test0490_3_D.png\n",
      "./test-images/test0491_1_B.png\n",
      "./test-images/test0492_4_E.png\n",
      "./test-images/test0493_17_R.png\n",
      "./test-images/test0494_24_Y.png\n",
      "./test-images/test0495_4_E.png\n",
      "./test-images/test0496_19_T.png\n",
      "./test-images/test0497_3_D.png\n",
      "./test-images/test0498_28_space.png\n",
      "./test-images/test0499_6_G.png\n",
      "./test-images/test0500_3_D.png\n",
      "./test-images/test0501_15_P.png\n",
      "./test-images/test0502_21_V.png\n",
      "./test-images/test0503_22_W.png\n",
      "./test-images/test0504_24_Y.png\n",
      "./test-images/test0505_1_B.png\n",
      "./test-images/test0506_25_Z.png\n",
      "./test-images/test0507_2_C.png\n",
      "./test-images/test0508_5_F.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "./test-images/test0509_3_D.png\n",
      "./test-images/test0510_4_E.png\n",
      "./test-images/test0511_1_B.png\n",
      "./test-images/test0512_8_I.png\n",
      "./test-images/test0513_22_W.png\n",
      "./test-images/test0514_7_H.png\n",
      "./test-images/test0515_17_R.png\n",
      "./test-images/test0516_10_K.png\n",
      "./test-images/test0517_13_N.png\n",
      "./test-images/test0518_5_F.png\n",
      "./test-images/test0519_0_A.png\n",
      "./test-images/test0520_24_Y.png\n",
      "./test-images/test0521_8_I.png\n",
      "./test-images/test0522_14_O.png\n",
      "./test-images/test0523_28_space.png\n",
      "./test-images/test0524_18_S.png\n",
      "./test-images/test0525_23_X.png\n",
      "./test-images/test0526_25_Z.png\n",
      "./test-images/test0527_21_V.png\n",
      "./test-images/test0528_7_H.png\n",
      "./test-images/test0529_0_A.png\n",
      "./test-images/test0530_4_E.png\n",
      "./test-images/test0531_11_L.png\n",
      "./test-images/test0532_25_Z.png\n",
      "./test-images/test0533_2_C.png\n",
      "./test-images/test0534_20_U.png\n",
      "./test-images/test0535_24_Y.png\n",
      "./test-images/test0536_26_del.png\n",
      "./test-images/test0537_26_del.png\n",
      "./test-images/test0538_10_K.png\n",
      "./test-images/test0539_4_E.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0540_21_V.png\n",
      "./test-images/test0541_12_M.png\n",
      "./test-images/test0542_8_I.png\n",
      "./test-images/test0543_27_nothing.png\n",
      "./test-images/test0544_18_S.png\n",
      "./test-images/test0545_20_U.png\n",
      "./test-images/test0546_14_O.png\n",
      "./test-images/test0547_24_Y.png\n",
      "./test-images/test0548_21_V.png\n",
      "./test-images/test0549_6_G.png\n",
      "./test-images/test0550_14_O.png\n",
      "./test-images/test0551_24_Y.png\n",
      "./test-images/test0552_22_W.png\n",
      "./test-images/test0553_27_nothing.png\n",
      "./test-images/test0554_8_I.png\n",
      "./test-images/test0555_7_H.png\n",
      "./test-images/test0556_0_A.png\n",
      "./test-images/test0557_4_E.png\n",
      "./test-images/test0558_7_H.png\n",
      "./test-images/test0559_22_W.png\n",
      "./test-images/test0560_28_space.png\n",
      "./test-images/test0561_28_space.png\n",
      "./test-images/test0562_26_del.png\n",
      "./test-images/test0563_28_space.png\n",
      "./test-images/test0564_20_U.png\n",
      "./test-images/test0565_6_G.png\n",
      "./test-images/test0566_16_Q.png\n",
      "./test-images/test0567_9_J.png\n",
      "./test-images/test0568_6_G.png\n",
      "./test-images/test0569_11_L.png\n",
      "./test-images/test0570_22_W.png\n",
      "./test-images/test0571_11_L.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0572_4_E.png\n",
      "./test-images/test0573_12_M.png\n",
      "./test-images/test0574_2_C.png\n",
      "./test-images/test0575_2_C.png\n",
      "./test-images/test0576_18_S.png\n",
      "./test-images/test0577_1_B.png\n",
      "./test-images/test0578_22_W.png\n",
      "./test-images/test0579_21_V.png\n",
      "./test-images/test0580_5_F.png\n",
      "./test-images/test0581_16_Q.png\n",
      "./test-images/test0582_2_C.png\n",
      "./test-images/test0583_14_O.png\n",
      "./test-images/test0584_10_K.png\n",
      "./test-images/test0585_14_O.png\n",
      "./test-images/test0586_10_K.png\n",
      "./test-images/test0587_23_X.png\n",
      "./test-images/test0588_21_V.png\n",
      "./test-images/test0589_16_Q.png\n",
      "./test-images/test0590_19_T.png\n",
      "./test-images/test0591_10_K.png\n",
      "./test-images/test0592_13_N.png\n",
      "./test-images/test0593_16_Q.png\n",
      "./test-images/test0594_23_X.png\n",
      "./test-images/test0595_14_O.png\n",
      "./test-images/test0596_19_T.png\n",
      "./test-images/test0597_5_F.png\n",
      "./test-images/test0598_19_T.png\n",
      "./test-images/test0599_27_nothing.png\n",
      "./test-images/test0600_21_V.png\n",
      "./test-images/test0601_15_P.png\n",
      "./test-images/test0602_24_Y.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0603_25_Z.png\n",
      "./test-images/test0604_28_space.png\n",
      "./test-images/test0605_1_B.png\n",
      "./test-images/test0606_25_Z.png\n",
      "./test-images/test0607_15_P.png\n",
      "./test-images/test0608_9_J.png\n",
      "./test-images/test0609_15_P.png\n",
      "./test-images/test0610_7_H.png\n",
      "./test-images/test0611_3_D.png\n",
      "./test-images/test0612_25_Z.png\n",
      "./test-images/test0613_11_L.png\n",
      "./test-images/test0614_18_S.png\n",
      "./test-images/test0615_6_G.png\n",
      "./test-images/test0616_16_Q.png\n",
      "./test-images/test0617_11_L.png\n",
      "./test-images/test0618_26_del.png\n",
      "./test-images/test0619_1_B.png\n",
      "./test-images/test0620_15_P.png\n",
      "./test-images/test0621_8_I.png\n",
      "./test-images/test0622_21_V.png\n",
      "./test-images/test0623_11_L.png\n",
      "./test-images/test0624_19_T.png\n",
      "./test-images/test0625_14_O.png\n",
      "./test-images/test0626_14_O.png\n",
      "./test-images/test0627_19_T.png\n",
      "./test-images/test0628_1_B.png\n",
      "./test-images/test0629_7_H.png\n",
      "./test-images/test0630_10_K.png\n",
      "./test-images/test0631_4_E.png\n",
      "./test-images/test0632_17_R.png\n",
      "./test-images/test0633_15_P.png\n",
      "./test-images/test0634_16_Q.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0635_8_I.png\n",
      "./test-images/test0636_13_N.png\n",
      "./test-images/test0637_7_H.png\n",
      "./test-images/test0638_26_del.png\n",
      "./test-images/test0639_0_A.png\n",
      "./test-images/test0640_15_P.png\n",
      "./test-images/test0641_7_H.png\n",
      "./test-images/test0642_11_L.png\n",
      "./test-images/test0643_6_G.png\n",
      "./test-images/test0644_12_M.png\n",
      "./test-images/test0645_2_C.png\n",
      "./test-images/test0646_27_nothing.png\n",
      "./test-images/test0647_24_Y.png\n",
      "./test-images/test0648_18_S.png\n",
      "./test-images/test0649_23_X.png\n",
      "./test-images/test0650_17_R.png\n",
      "./test-images/test0651_13_N.png\n",
      "./test-images/test0652_15_P.png\n",
      "./test-images/test0653_12_M.png\n",
      "./test-images/test0654_21_V.png\n",
      "./test-images/test0655_28_space.png\n",
      "./test-images/test0656_9_J.png\n",
      "./test-images/test0657_1_B.png\n",
      "./test-images/test0658_17_R.png\n",
      "./test-images/test0659_20_U.png\n",
      "./test-images/test0660_5_F.png\n",
      "./test-images/test0661_10_K.png\n",
      "./test-images/test0662_26_del.png\n",
      "./test-images/test0663_3_D.png\n",
      "./test-images/test0664_2_C.png\n",
      "./test-images/test0665_13_N.png\n",
      "./test-images/test0666_1_B.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0667_20_U.png\n",
      "./test-images/test0668_5_F.png\n",
      "./test-images/test0669_1_B.png\n",
      "./test-images/test0670_0_A.png\n",
      "./test-images/test0671_23_X.png\n",
      "./test-images/test0672_1_B.png\n",
      "./test-images/test0673_20_U.png\n",
      "./test-images/test0674_23_X.png\n",
      "./test-images/test0675_14_O.png\n",
      "./test-images/test0676_19_T.png\n",
      "./test-images/test0677_13_N.png\n",
      "./test-images/test0678_24_Y.png\n",
      "./test-images/test0679_2_C.png\n",
      "./test-images/test0680_12_M.png\n",
      "./test-images/test0681_2_C.png\n",
      "./test-images/test0682_26_del.png\n",
      "./test-images/test0683_2_C.png\n",
      "./test-images/test0684_19_T.png\n",
      "./test-images/test0685_26_del.png\n",
      "./test-images/test0686_6_G.png\n",
      "./test-images/test0687_16_Q.png\n",
      "./test-images/test0688_7_H.png\n",
      "./test-images/test0689_11_L.png\n",
      "./test-images/test0690_24_Y.png\n",
      "./test-images/test0691_20_U.png\n",
      "./test-images/test0692_24_Y.png\n",
      "./test-images/test0693_0_A.png\n",
      "./test-images/test0694_9_J.png\n",
      "./test-images/test0695_16_Q.png\n",
      "./test-images/test0696_7_H.png\n",
      "./test-images/test0697_16_Q.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0698_24_Y.png\n",
      "./test-images/test0699_21_V.png\n",
      "./test-images/test0700_17_R.png\n",
      "./test-images/test0701_0_A.png\n",
      "./test-images/test0702_11_L.png\n",
      "./test-images/test0703_9_J.png\n",
      "./test-images/test0704_1_B.png\n",
      "./test-images/test0705_26_del.png\n",
      "./test-images/test0706_3_D.png\n",
      "./test-images/test0707_24_Y.png\n",
      "./test-images/test0708_2_C.png\n",
      "./test-images/test0709_24_Y.png\n",
      "./test-images/test0710_8_I.png\n",
      "./test-images/test0711_5_F.png\n",
      "./test-images/test0712_5_F.png\n",
      "./test-images/test0713_14_O.png\n",
      "./test-images/test0714_0_A.png\n",
      "./test-images/test0715_25_Z.png\n",
      "./test-images/test0716_6_G.png\n",
      "./test-images/test0717_10_K.png\n",
      "./test-images/test0718_6_G.png\n",
      "./test-images/test0719_4_E.png\n",
      "./test-images/test0720_21_V.png\n",
      "./test-images/test0721_6_G.png\n",
      "./test-images/test0722_3_D.png\n",
      "./test-images/test0723_5_F.png\n",
      "./test-images/test0724_10_K.png\n",
      "./test-images/test0725_23_X.png\n",
      "./test-images/test0726_16_Q.png\n",
      "./test-images/test0727_6_G.png\n",
      "./test-images/test0728_23_X.png\n",
      "./test-images/test0729_17_R.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0730_15_P.png\n",
      "./test-images/test0731_18_S.png\n",
      "./test-images/test0732_21_V.png\n",
      "./test-images/test0733_21_V.png\n",
      "./test-images/test0734_0_A.png\n",
      "./test-images/test0735_5_F.png\n",
      "./test-images/test0736_5_F.png\n",
      "./test-images/test0737_13_N.png\n",
      "./test-images/test0738_9_J.png\n",
      "./test-images/test0739_24_Y.png\n",
      "./test-images/test0740_25_Z.png\n",
      "./test-images/test0741_7_H.png\n",
      "./test-images/test0742_4_E.png\n",
      "./test-images/test0743_28_space.png\n",
      "./test-images/test0744_14_O.png\n",
      "./test-images/test0745_12_M.png\n",
      "./test-images/test0746_6_G.png\n",
      "./test-images/test0747_26_del.png\n",
      "./test-images/test0748_9_J.png\n",
      "./test-images/test0749_0_A.png\n",
      "./test-images/test0750_21_V.png\n",
      "./test-images/test0751_12_M.png\n",
      "./test-images/test0752_1_B.png\n",
      "./test-images/test0753_26_del.png\n",
      "./test-images/test0754_13_N.png\n",
      "./test-images/test0755_22_W.png\n",
      "./test-images/test0756_17_R.png\n",
      "./test-images/test0757_2_C.png\n",
      "./test-images/test0758_19_T.png\n",
      "./test-images/test0759_28_space.png\n",
      "./test-images/test0760_1_B.png\n",
      "./test-images/test0761_2_C.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0762_4_E.png\n",
      "./test-images/test0763_19_T.png\n",
      "./test-images/test0764_7_H.png\n",
      "./test-images/test0765_28_space.png\n",
      "./test-images/test0766_7_H.png\n",
      "./test-images/test0767_27_nothing.png\n",
      "./test-images/test0768_6_G.png\n",
      "./test-images/test0769_22_W.png\n",
      "./test-images/test0770_2_C.png\n",
      "./test-images/test0771_0_A.png\n",
      "./test-images/test0772_6_G.png\n",
      "./test-images/test0773_8_I.png\n",
      "./test-images/test0774_28_space.png\n",
      "./test-images/test0775_19_T.png\n",
      "./test-images/test0776_27_nothing.png\n",
      "./test-images/test0777_25_Z.png\n",
      "./test-images/test0778_12_M.png\n",
      "./test-images/test0779_3_D.png\n",
      "./test-images/test0780_27_nothing.png\n",
      "./test-images/test0781_27_nothing.png\n",
      "./test-images/test0782_6_G.png\n",
      "./test-images/test0783_28_space.png\n",
      "./test-images/test0784_17_R.png\n",
      "./test-images/test0785_5_F.png\n",
      "./test-images/test0786_3_D.png\n",
      "./test-images/test0787_13_N.png\n",
      "./test-images/test0788_27_nothing.png\n",
      "./test-images/test0789_10_K.png\n",
      "./test-images/test0790_27_nothing.png\n",
      "./test-images/test0791_21_V.png\n",
      "./test-images/test0792_28_space.png\n",
      "./test-images/test0793_0_A.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0794_24_Y.png\n",
      "./test-images/test0795_23_X.png\n",
      "./test-images/test0796_15_P.png\n",
      "./test-images/test0797_8_I.png\n",
      "./test-images/test0798_22_W.png\n",
      "./test-images/test0799_27_nothing.png\n",
      "./test-images/test0800_28_space.png\n",
      "./test-images/test0801_10_K.png\n",
      "./test-images/test0802_1_B.png\n",
      "./test-images/test0803_8_I.png\n",
      "./test-images/test0804_19_T.png\n",
      "./test-images/test0805_18_S.png\n",
      "./test-images/test0806_19_T.png\n",
      "./test-images/test0807_8_I.png\n",
      "./test-images/test0808_13_N.png\n",
      "./test-images/test0809_3_D.png\n",
      "./test-images/test0810_3_D.png\n",
      "./test-images/test0811_14_O.png\n",
      "./test-images/test0812_28_space.png\n",
      "./test-images/test0813_15_P.png\n",
      "./test-images/test0814_24_Y.png\n",
      "./test-images/test0815_7_H.png\n",
      "./test-images/test0816_11_L.png\n",
      "./test-images/test0817_20_U.png\n",
      "./test-images/test0818_12_M.png\n",
      "./test-images/test0819_6_G.png\n",
      "./test-images/test0820_28_space.png\n",
      "./test-images/test0821_11_L.png\n",
      "./test-images/test0822_27_nothing.png\n",
      "./test-images/test0823_11_L.png\n",
      "./test-images/test0824_16_Q.png\n",
      "./test-images/test0825_15_P.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0826_14_O.png\n",
      "./test-images/test0827_2_C.png\n",
      "./test-images/test0828_0_A.png\n",
      "./test-images/test0829_8_I.png\n",
      "./test-images/test0830_28_space.png\n",
      "./test-images/test0831_15_P.png\n",
      "./test-images/test0832_10_K.png\n",
      "./test-images/test0833_10_K.png\n",
      "./test-images/test0834_14_O.png\n",
      "./test-images/test0835_1_B.png\n",
      "./test-images/test0836_17_R.png\n",
      "./test-images/test0837_9_J.png\n",
      "./test-images/test0838_6_G.png\n",
      "./test-images/test0839_10_K.png\n",
      "./test-images/test0840_19_T.png\n",
      "./test-images/test0841_9_J.png\n",
      "./test-images/test0842_28_space.png\n",
      "./test-images/test0843_22_W.png\n",
      "./test-images/test0844_18_S.png\n",
      "./test-images/test0845_6_G.png\n",
      "./test-images/test0846_25_Z.png\n",
      "./test-images/test0847_26_del.png\n",
      "./test-images/test0848_18_S.png\n",
      "./test-images/test0849_18_S.png\n",
      "./test-images/test0850_21_V.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "generate_test_images(valid_dataset, TrainingConfig.CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78e2b5",
   "metadata": {},
   "source": [
    "## 6 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3256950",
   "metadata": {},
   "source": [
    "In this notebook, we showed how to quantize and compile a TensorFlow2 model with Vitis-AI for deployment on AMD Zynq-UltraScale+ devices. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "c4_03_15a_ASL150_VGG_TransferLearn_FineTune_Conv2D_Train_Dense.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
