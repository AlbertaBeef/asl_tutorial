{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2321d8cb",
   "metadata": {
    "id": "2321d8cb"
   },
   "source": [
    "<h1 style=\"font-size:30px;\">Deploying the ASL Classifier to Vitis-AI</h1>  \n",
    "\n",
    "This notebook describes how to quantize and compile a TensorFlow2 model with Vitis-AI for deployment.\n",
    "\n",
    "<img src='./images/VGG16_06_asl_fine_tuning.png' width=1000 align='center'><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f452f7",
   "metadata": {
    "id": "f5f452f7"
   },
   "source": [
    "## Table of Contents\n",
    "* [1 System Configuration](#1-System-Configuration)\n",
    "* [2 Download and Extract the Dataset](#2-Download-and-Extract-the-Dataset)\n",
    "* [3 Dataset Configuration](#3-Dataset-Configuration)\n",
    "* [4 Quantization](#4-Quantization)\n",
    "* [5 Compilation](#5-Compilation)\n",
    "* [6 Conclusion](#6-Conclusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d103ad0",
   "metadata": {
    "id": "0d103ad0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 15:09:43.392532: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-21 15:09:50.498405: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import zipfile\n",
    "import requests\n",
    "import glob as glob\n",
    "\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter)\n",
    "from dataclasses import dataclass \n",
    "\n",
    "block_plot = False\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "SEED_VALUE = 42 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42883a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version :  2.10.0\n",
      "tensorflow version :  2.10.0\n",
      "opencv version :  4.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow version : \",tf.__version__)\n",
    "print(\"tensorflow version : \",keras.__version__)\n",
    "print(\"opencv version : \",cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61afbd",
   "metadata": {
    "id": "bc61afbd"
   },
   "source": [
    "## 1 System Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450b9a9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "450b9a9a",
    "outputId": "4b0ff0e3-fc7d-4689-a8f4-953b7b7fa392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "def system_config():\n",
    "    \n",
    "    # Get list of GPUs.\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    print(gpu_devices)\n",
    "    \n",
    "    if len(gpu_devices) > 0:\n",
    "        print('Using GPU')\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "        os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n",
    "        \n",
    "        # If there are any gpu devices, use first gpu.\n",
    "        tf.config.experimental.set_visible_devices(gpu_devices[0], 'GPU')\n",
    "        \n",
    "        # Grow the memory usage as it is needed by the process.\n",
    "        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "        \n",
    "        # Enable using cudNN.\n",
    "        os.environ['TF_USE_CUDNN'] = \"true\"\n",
    "    else:\n",
    "        print('Using CPU')\n",
    "\n",
    "system_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfe4cd",
   "metadata": {
    "id": "97cfe4cd"
   },
   "source": [
    "## 2 Download and Extract the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c6b79b",
   "metadata": {
    "id": "03c6b79b"
   },
   "outputs": [],
   "source": [
    "def download_file(url, save_name):\n",
    "    url = url\n",
    "    file = requests.get(url)\n",
    "\n",
    "    open(save_name, 'wb').write(file.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf238d22",
   "metadata": {
    "id": "bf238d22"
   },
   "outputs": [],
   "source": [
    "def unzip(zip_file=None):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_file) as z:\n",
    "            z.extractall(\"./\")\n",
    "            print(\"Extracted all\")\n",
    "    except:\n",
    "        print(\"Invalid file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55f0cb6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55f0cb6a",
    "outputId": "0e54ae7b-cb38-4469-9848-baa3addb5d47"
   },
   "outputs": [],
   "source": [
    "#download_file(\n",
    "#    'https://www.dropbox.com/s/7huaqeavdbz32la/dataset_ASL_150.zip?dl=1', \n",
    "#    'dataset_ASL_150.zip'\n",
    "#)\n",
    "#    \n",
    "#unzip(zip_file='dataset_ASL_150.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8158104",
   "metadata": {
    "id": "d8158104"
   },
   "source": [
    "## 3 Dataset and Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b8e05f",
   "metadata": {
    "id": "a9b8e05f"
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    NUM_CLASSES: int = 29\n",
    "    IMG_HEIGHT:  int = 224\n",
    "    IMG_WIDTH:   int = 224\n",
    "    CHANNELS:    int = 3\n",
    "    BATCH_SIZE:  int = 32\n",
    "    DATA_ROOT:   str = './dataset_ASL_reduced'\n",
    "        \n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE:     int   = 32\n",
    "    EPOCHS:         int   = 51\n",
    "    LEARNING_RATE:  float = 0.0001\n",
    "    CHECKPOINT_DIR: str   = './saved_models_asl_classifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddde6491",
   "metadata": {
    "id": "ddde6491"
   },
   "source": [
    "### 3.1 Prepare the Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927f1028",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "927f1028",
    "outputId": "85c82008-4818-45a3-819d-0622b7c8db1a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5800 files belonging to 29 classes.\n",
      "Using 4640 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 15:11:25.345154: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5800 files belonging to 29 classes.\n",
      "Using 1160 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(directory=DatasetConfig.DATA_ROOT,\n",
    "                                             batch_size=TrainingConfig.BATCH_SIZE,\n",
    "                                             shuffle=True,\n",
    "                                             seed=SEED_VALUE,\n",
    "                                             label_mode='categorical',\n",
    "                                             image_size=(DatasetConfig.IMG_WIDTH, DatasetConfig.IMG_HEIGHT),\n",
    "                                             subset='training',\n",
    "                                             validation_split=0.2\n",
    "                                            )\n",
    "\n",
    "valid_dataset = image_dataset_from_directory(directory=DatasetConfig.DATA_ROOT,\n",
    "                                             batch_size=TrainingConfig.BATCH_SIZE,\n",
    "                                             shuffle=True,\n",
    "                                             seed=SEED_VALUE,\n",
    "                                             label_mode='categorical',\n",
    "                                             image_size=(DatasetConfig.IMG_WIDTH, DatasetConfig.IMG_HEIGHT),\n",
    "                                             subset='validation',\n",
    "                                             validation_split=0.2\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960fa33c",
   "metadata": {
    "id": "960fa33c"
   },
   "source": [
    "## 4 Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042806bf",
   "metadata": {},
   "source": [
    "**Load model**\n",
    "\n",
    "Load model for the rest of the tutorial with the `load_model` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f10bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.load_model('tf2_asl_classifier3.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d699bab",
   "metadata": {
    "id": "cadbd8a6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17052db7",
   "metadata": {},
   "source": [
    "In order to compile the trained model for deployment on a DPU platform, we must first quantize it. Here we will use the `vitis_quantize` module to convert the floating point model into an INT8 quantized representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab4b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.quantization.keras import vitis_quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f0c299",
   "metadata": {},
   "source": [
    "**Quantize model**\n",
    "\n",
    "By default the `quantize_model` function converts the weights, activations and inputs into 8-bit wide numbers. We can specify different values and configurations using `weight_bit`, `activation_bit` and other parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3ec24d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAI INFO] Update activation_bit: 8\n",
      "[VAI INFO] Update weight_bit: 8\n",
      "[VAI INFO] Quantizing without specific `target`.\n",
      "[VAI INFO] Start CrossLayerEqualization...\n",
      "10/10 [==============================] - 5s 536ms/step\n",
      "[VAI INFO] CrossLayerEqualization Done.\n",
      "[VAI INFO] Start Quantize Calibration...\n",
      "37/37 [==============================] - 220s 6s/step\n",
      "[VAI INFO] Quantize Calibration Done.\n",
      "[VAI INFO] Start Post-Quant Model Refinement...\n",
      "[VAI INFO] Start Quantize Position Ajustment...\n",
      "[VAI INFO] Quantize Position Ajustment Done.\n",
      "[VAI INFO] Post-Quant Model Refninement Done.\n",
      "[VAI INFO] Start Model Finalization...\n",
      "[VAI INFO] Model Finalization Done.\n",
      "[VAI INFO] Quantization Finished.\n"
     ]
    }
   ],
   "source": [
    "quantizer = vitis_quantize.VitisQuantizer(model)\n",
    "quantized_model = quantizer.quantize_model(calib_dataset=valid_dataset, weight_bit=8, activation_bit=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0236c8a",
   "metadata": {},
   "source": [
    "**Evaluate quantized model**\n",
    "\n",
    "In order to evaluate the quantized model, it needs to be re-compiled with the desired loss and evaluation metrics, such as accuracy. Since we are using 8-bit quantization we do not lose much performance, if at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26efd672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 88s 2s/step - loss: 0.1012 - accuracy: 0.9784\n",
      "Model evaluation accuracy: 97.845\n"
     ]
    }
   ],
   "source": [
    "quantized_model.compile(loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "print(f\"Model evaluation accuracy: {quantized_model.evaluate(valid_dataset)[1]*100.:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb228f43",
   "metadata": {},
   "source": [
    "**Save quantized model**\n",
    "\n",
    "Once we are happy with the performance of the quantized model, we can save it as a .h5 file, simply using the `save` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb4a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model.save('tf2_asl_classifier_quantized.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58451641",
   "metadata": {},
   "source": [
    "## 5 Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7974c603",
   "metadata": {},
   "source": [
    "For this final step we use the Vitis AI compiler `vai_c_tensorflow2` and pass the quantized model as a parameter. \n",
    "\n",
    "The target platform (ie. specific DPU architecture) is defined by .arch file.\n",
    "\n",
    "To support as many platforms as possible, we compile for the following DPU architectures:\n",
    "- B4096 (ZCU102, ZCU104, UltraZed-EV)\n",
    "- B3136 (KV260)\n",
    "- B2304 (Ultra96-V2)\n",
    "- B1152 (Ultra96-V2+DualCam)\n",
    "-  B512 (ZUBoard)\n",
    "-  B128 (ZUBoard+DualCam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ecf09d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_DPUCZDX8G_ISA1_B4096_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 25408.18it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 857.61it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 190.14it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 755.93it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 263.26it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_DPUCZDX8G_ISA1_B4096_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B4096\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B4096/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B4096//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 25de9aa1b55fc6eb5aa02e1a754f9981, and has been saved to \"/workspace/./model/B4096/md5sum.txt\"\n",
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_DPUCZDX8G_ISA1_B3136_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 18106.91it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 777.03it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 188.02it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 751.94it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 258.85it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_DPUCZDX8G_ISA1_B3136_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B3136\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B3136/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B3136//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 3c15f6c2743be2082a70d35c1ef27d1f, and has been saved to \"/workspace/./model/B3136/md5sum.txt\"\n",
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_0x101000016010405_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 18054.95it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 870.82it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 188.92it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 759.98it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 259.22it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_0x101000016010405_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B2304_0101000016010405\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B2304/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B2304//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is fc08d46a178d8bbd80cae1a34487b900, and has been saved to \"/workspace/./model/B2304/md5sum.txt\"\n",
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_0x101000017010203_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 24871.20it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 878.51it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 190.01it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 749.92it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 258.46it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_0x101000017010203_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B1152_0101000017010203\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B1152/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B1152//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 039a4dcc3bd0365398801e4a6da40f9d, and has been saved to \"/workspace/./model/B1152/md5sum.txt\"\n",
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_0x101000016010200_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 25200.72it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 848.11it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 185.24it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 726.66it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 249.37it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_0x101000016010200_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B512_0101000016010200\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B512/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B512//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 8095eebf3ef1efafc28fcc9a2efd96d9, and has been saved to \"/workspace/./model/B512/md5sum.txt\"\n",
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./tf2_asl_classifier_quantized.h5'], model_type='tensorflow2', named_inputs_shape=None, out_filename='/tmp/asl_classifier_0x101000002010208_org.xmodel', proto=None)\n",
      "[INFO] tensorflow2 model: /workspace/tf2_asl_classifier_quantized.h5\n",
      "[INFO] keras version: 2.10.0\n",
      "[INFO] Tensorflow Keras model type: functional\n",
      "[INFO] parse raw model     :100%|█| 39/39 [00:00<00:00, 25364.84it/s]           \n",
      "[INFO] infer shape (NHWC)  :100%|█| 64/64 [00:00<00:00, 849.58it/s]             \n",
      "[INFO] perform level-0 opt :100%|█| 1/1 [00:00<00:00, 185.39it/s]               \n",
      "[INFO] perform level-1 opt :100%|█| 2/2 [00:00<00:00, 748.38it/s]               \n",
      "[INFO] generate xmodel     :100%|█| 64/64 [00:00<00:00, 258.47it/s]             \n",
      "[INFO] dump xmodel: /tmp/asl_classifier_0x101000002010208_org.xmodel\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: null\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA1_B128_0101000002010208\n",
      "[UNILOG][INFO] Graph name: model, with op num: 120\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/./model/B128/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/./model/B128//asl_classifier.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 047083f2d9954123bb7fddd88fc468e6, and has been saved to \"/workspace/./model/B128/md5sum.txt\"\n"
     ]
    }
   ],
   "source": [
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier_quantized.h5 \\\n",
    "    --arch ./arch/B4096/arch-zcu104.json \\\n",
    "    --output_dir ./model/B4096/ \\\n",
    "    --net_name asl_classifier\n",
    "\n",
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier_quantized.h5 \\\n",
    "    --arch ./arch/B3136/arch-kv260.json \\\n",
    "    --output_dir ./model/B3136/ \\\n",
    "    --net_name asl_classifier\n",
    "\n",
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier_quantized.h5 \\\n",
    "    --arch ./arch/B2304/arch-b2304-lr.json \\\n",
    "    --output_dir ./model/B2304/ \\\n",
    "    --net_name asl_classifier\n",
    "\n",
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier_quantized.h5 \\\n",
    "    --arch ./arch/B1152/arch-b1152-hr.json \\\n",
    "    --output_dir ./model/B1152/ \\\n",
    "    --net_name asl_classifier\n",
    "\n",
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier_quantized.h5 \\\n",
    "    --arch ./arch/B512/arch-b512-lr.json \\\n",
    "    --output_dir ./model/B512/ \\\n",
    "    --net_name asl_classifier\n",
    "\n",
    "!vai_c_tensorflow2 \\\n",
    "    --model ./tf2_asl_classifier_quantized.h5 \\\n",
    "    --arch ./arch/B128/arch-b128-lr.json \\\n",
    "    --output_dir ./model/B128/ \\\n",
    "    --net_name asl_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4d4699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4954c68",
   "metadata": {},
   "source": [
    "### Generate test-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87e332c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './test-images'\n",
    "\n",
    "   \n",
    "def generate_test_images(dataset, checkpoint_dir=None, checkpoint_version=0):\n",
    "    \n",
    "    if not checkpoint_dir:\n",
    "        checkpoint_dir = os.path.join(os.getcwd(), TrainingConfig.checkpoint_dir, f\"version_{checkpoint_version}\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "         \n",
    "    # Load saved model.\n",
    "    model = tf.keras.models.load_model(checkpoint_dir)\n",
    "    \n",
    "    num_test_images = 1024\n",
    "    class_names = dataset.class_names\n",
    "    jdx = 0\n",
    "    \n",
    "    # Evaluate all the batches.\n",
    "    for image_batch, labels_batch in dataset:\n",
    "        \n",
    "        # Predictions for the current batch.\n",
    "        predictions = model.predict(image_batch)\n",
    "        \n",
    "        # Loop over all the images in the current batch.\n",
    "        for idx in range(len(labels_batch)):\n",
    "            \n",
    "            pred_idx = tf.argmax(predictions[idx]).numpy()\n",
    "            truth_idx = np.nonzero(labels_batch[idx].numpy())\n",
    "            \n",
    "            # Plot the images with incorrect predictions\n",
    "            if pred_idx == truth_idx:\n",
    "                \n",
    "                jdx += 1\n",
    "                \n",
    "                if jdx > num_test_images:\n",
    "                    # Break from the loops if the maximum number of images have been plotted\n",
    "                    break\n",
    "                \n",
    "                image = image_batch[idx].numpy().astype(\"uint8\")\n",
    "                image_dst = output_dir+\"/test%04d\"%(jdx)+'_'+str(pred_idx)+'_'+str(class_names[pred_idx])+'.png'\n",
    "                if not os.path.exists(image_dst):\n",
    "                    print(image_dst)\n",
    "                    cv2.imwrite(image_dst, image )\n",
    "            \n",
    "    return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "231a65e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0001_0_A.png\n",
      "./test-images/test0002_23_X.png\n",
      "./test-images/test0003_14_O.png\n",
      "./test-images/test0004_19_T.png\n",
      "./test-images/test0005_17_R.png\n",
      "./test-images/test0006_21_V.png\n",
      "./test-images/test0007_5_F.png\n",
      "./test-images/test0008_0_A.png\n",
      "./test-images/test0009_14_O.png\n",
      "./test-images/test0010_25_Z.png\n",
      "./test-images/test0011_18_S.png\n",
      "./test-images/test0012_20_U.png\n",
      "./test-images/test0013_9_J.png\n",
      "./test-images/test0014_18_S.png\n",
      "./test-images/test0015_9_J.png\n",
      "./test-images/test0016_12_M.png\n",
      "./test-images/test0017_20_U.png\n",
      "./test-images/test0018_2_C.png\n",
      "./test-images/test0019_13_N.png\n",
      "./test-images/test0020_5_F.png\n",
      "./test-images/test0021_23_X.png\n",
      "./test-images/test0022_5_F.png\n",
      "./test-images/test0023_14_O.png\n",
      "./test-images/test0024_5_F.png\n",
      "./test-images/test0025_25_Z.png\n",
      "./test-images/test0026_24_Y.png\n",
      "./test-images/test0027_18_S.png\n",
      "./test-images/test0028_6_G.png\n",
      "./test-images/test0029_9_J.png\n",
      "./test-images/test0030_10_K.png\n",
      "./test-images/test0031_20_U.png\n",
      "./test-images/test0032_17_R.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0033_25_Z.png\n",
      "./test-images/test0034_4_E.png\n",
      "./test-images/test0035_6_G.png\n",
      "./test-images/test0036_26_del.png\n",
      "./test-images/test0037_24_Y.png\n",
      "./test-images/test0038_16_Q.png\n",
      "./test-images/test0039_27_nothing.png\n",
      "./test-images/test0040_11_L.png\n",
      "./test-images/test0041_16_Q.png\n",
      "./test-images/test0042_20_U.png\n",
      "./test-images/test0043_24_Y.png\n",
      "./test-images/test0044_1_B.png\n",
      "./test-images/test0045_3_D.png\n",
      "./test-images/test0046_1_B.png\n",
      "./test-images/test0047_26_del.png\n",
      "./test-images/test0048_19_T.png\n",
      "./test-images/test0049_22_W.png\n",
      "./test-images/test0050_4_E.png\n",
      "./test-images/test0051_6_G.png\n",
      "./test-images/test0052_9_J.png\n",
      "./test-images/test0053_1_B.png\n",
      "./test-images/test0054_21_V.png\n",
      "./test-images/test0055_20_U.png\n",
      "./test-images/test0056_6_G.png\n",
      "./test-images/test0057_3_D.png\n",
      "./test-images/test0058_0_A.png\n",
      "./test-images/test0059_20_U.png\n",
      "./test-images/test0060_18_S.png\n",
      "./test-images/test0061_7_H.png\n",
      "./test-images/test0062_2_C.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0063_16_Q.png\n",
      "./test-images/test0064_16_Q.png\n",
      "./test-images/test0065_22_W.png\n",
      "./test-images/test0066_4_E.png\n",
      "./test-images/test0067_8_I.png\n",
      "./test-images/test0068_12_M.png\n",
      "./test-images/test0069_10_K.png\n",
      "./test-images/test0070_24_Y.png\n",
      "./test-images/test0071_12_M.png\n",
      "./test-images/test0072_21_V.png\n",
      "./test-images/test0073_8_I.png\n",
      "./test-images/test0074_4_E.png\n",
      "./test-images/test0075_7_H.png\n",
      "./test-images/test0076_10_K.png\n",
      "./test-images/test0077_28_space.png\n",
      "./test-images/test0078_19_T.png\n",
      "./test-images/test0079_19_T.png\n",
      "./test-images/test0080_20_U.png\n",
      "./test-images/test0081_2_C.png\n",
      "./test-images/test0082_0_A.png\n",
      "./test-images/test0083_20_U.png\n",
      "./test-images/test0084_7_H.png\n",
      "./test-images/test0085_19_T.png\n",
      "./test-images/test0086_13_N.png\n",
      "./test-images/test0087_16_Q.png\n",
      "./test-images/test0088_0_A.png\n",
      "./test-images/test0089_22_W.png\n",
      "./test-images/test0090_4_E.png\n",
      "./test-images/test0091_20_U.png\n",
      "./test-images/test0092_18_S.png\n",
      "./test-images/test0093_26_del.png\n",
      "./test-images/test0094_22_W.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0095_8_I.png\n",
      "./test-images/test0096_26_del.png\n",
      "./test-images/test0097_4_E.png\n",
      "./test-images/test0098_9_J.png\n",
      "./test-images/test0099_1_B.png\n",
      "./test-images/test0100_14_O.png\n",
      "./test-images/test0101_2_C.png\n",
      "./test-images/test0102_28_space.png\n",
      "./test-images/test0103_8_I.png\n",
      "./test-images/test0104_20_U.png\n",
      "./test-images/test0105_27_nothing.png\n",
      "./test-images/test0106_28_space.png\n",
      "./test-images/test0107_4_E.png\n",
      "./test-images/test0108_23_X.png\n",
      "./test-images/test0109_8_I.png\n",
      "./test-images/test0110_18_S.png\n",
      "./test-images/test0111_23_X.png\n",
      "./test-images/test0112_26_del.png\n",
      "./test-images/test0113_23_X.png\n",
      "./test-images/test0114_25_Z.png\n",
      "./test-images/test0115_20_U.png\n",
      "./test-images/test0116_16_Q.png\n",
      "./test-images/test0117_20_U.png\n",
      "./test-images/test0118_26_del.png\n",
      "./test-images/test0119_0_A.png\n",
      "./test-images/test0120_9_J.png\n",
      "./test-images/test0121_25_Z.png\n",
      "./test-images/test0122_2_C.png\n",
      "./test-images/test0123_18_S.png\n",
      "./test-images/test0124_24_Y.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0125_19_T.png\n",
      "./test-images/test0126_23_X.png\n",
      "./test-images/test0127_12_M.png\n",
      "./test-images/test0128_20_U.png\n",
      "./test-images/test0129_28_space.png\n",
      "./test-images/test0130_1_B.png\n",
      "./test-images/test0131_14_O.png\n",
      "./test-images/test0132_18_S.png\n",
      "./test-images/test0133_7_H.png\n",
      "./test-images/test0134_18_S.png\n",
      "./test-images/test0135_18_S.png\n",
      "./test-images/test0136_26_del.png\n",
      "./test-images/test0137_19_T.png\n",
      "./test-images/test0138_0_A.png\n",
      "./test-images/test0139_6_G.png\n",
      "./test-images/test0140_11_L.png\n",
      "./test-images/test0141_19_T.png\n",
      "./test-images/test0142_27_nothing.png\n",
      "./test-images/test0143_22_W.png\n",
      "./test-images/test0144_11_L.png\n",
      "./test-images/test0145_9_J.png\n",
      "./test-images/test0146_25_Z.png\n",
      "./test-images/test0147_6_G.png\n",
      "./test-images/test0148_20_U.png\n",
      "./test-images/test0149_28_space.png\n",
      "./test-images/test0150_12_M.png\n",
      "./test-images/test0151_18_S.png\n",
      "./test-images/test0152_25_Z.png\n",
      "./test-images/test0153_14_O.png\n",
      "./test-images/test0154_11_L.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0155_6_G.png\n",
      "./test-images/test0156_16_Q.png\n",
      "./test-images/test0157_26_del.png\n",
      "./test-images/test0158_9_J.png\n",
      "./test-images/test0159_23_X.png\n",
      "./test-images/test0160_26_del.png\n",
      "./test-images/test0161_10_K.png\n",
      "./test-images/test0162_10_K.png\n",
      "./test-images/test0163_17_R.png\n",
      "./test-images/test0164_0_A.png\n",
      "./test-images/test0165_2_C.png\n",
      "./test-images/test0166_25_Z.png\n",
      "./test-images/test0167_19_T.png\n",
      "./test-images/test0168_22_W.png\n",
      "./test-images/test0169_11_L.png\n",
      "./test-images/test0170_28_space.png\n",
      "./test-images/test0171_26_del.png\n",
      "./test-images/test0172_5_F.png\n",
      "./test-images/test0173_25_Z.png\n",
      "./test-images/test0174_14_O.png\n",
      "./test-images/test0175_23_X.png\n",
      "./test-images/test0176_18_S.png\n",
      "./test-images/test0177_26_del.png\n",
      "./test-images/test0178_5_F.png\n",
      "./test-images/test0179_3_D.png\n",
      "./test-images/test0180_18_S.png\n",
      "./test-images/test0181_17_R.png\n",
      "./test-images/test0182_3_D.png\n",
      "./test-images/test0183_10_K.png\n",
      "./test-images/test0184_18_S.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0185_18_S.png\n",
      "./test-images/test0186_9_J.png\n",
      "./test-images/test0187_8_I.png\n",
      "./test-images/test0188_24_Y.png\n",
      "./test-images/test0189_22_W.png\n",
      "./test-images/test0190_18_S.png\n",
      "./test-images/test0191_21_V.png\n",
      "./test-images/test0192_16_Q.png\n",
      "./test-images/test0193_24_Y.png\n",
      "./test-images/test0194_22_W.png\n",
      "./test-images/test0195_3_D.png\n",
      "./test-images/test0196_18_S.png\n",
      "./test-images/test0197_20_U.png\n",
      "./test-images/test0198_10_K.png\n",
      "./test-images/test0199_16_Q.png\n",
      "./test-images/test0200_20_U.png\n",
      "./test-images/test0201_27_nothing.png\n",
      "./test-images/test0202_16_Q.png\n",
      "./test-images/test0203_12_M.png\n",
      "./test-images/test0204_16_Q.png\n",
      "./test-images/test0205_7_H.png\n",
      "./test-images/test0206_23_X.png\n",
      "./test-images/test0207_22_W.png\n",
      "./test-images/test0208_25_Z.png\n",
      "./test-images/test0209_10_K.png\n",
      "./test-images/test0210_18_S.png\n",
      "./test-images/test0211_5_F.png\n",
      "./test-images/test0212_28_space.png\n",
      "./test-images/test0213_8_I.png\n",
      "./test-images/test0214_13_N.png\n",
      "./test-images/test0215_3_D.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0216_25_Z.png\n",
      "./test-images/test0217_24_Y.png\n",
      "./test-images/test0218_13_N.png\n",
      "./test-images/test0219_4_E.png\n",
      "./test-images/test0220_26_del.png\n",
      "./test-images/test0221_12_M.png\n",
      "./test-images/test0222_24_Y.png\n",
      "./test-images/test0223_14_O.png\n",
      "./test-images/test0224_11_L.png\n",
      "./test-images/test0225_19_T.png\n",
      "./test-images/test0226_20_U.png\n",
      "./test-images/test0227_7_H.png\n",
      "./test-images/test0228_4_E.png\n",
      "./test-images/test0229_11_L.png\n",
      "./test-images/test0230_10_K.png\n",
      "./test-images/test0231_25_Z.png\n",
      "./test-images/test0232_23_X.png\n",
      "./test-images/test0233_25_Z.png\n",
      "./test-images/test0234_25_Z.png\n",
      "./test-images/test0235_22_W.png\n",
      "./test-images/test0236_21_V.png\n",
      "./test-images/test0237_4_E.png\n",
      "./test-images/test0238_7_H.png\n",
      "./test-images/test0239_22_W.png\n",
      "./test-images/test0240_13_N.png\n",
      "./test-images/test0241_19_T.png\n",
      "./test-images/test0242_10_K.png\n",
      "./test-images/test0243_7_H.png\n",
      "./test-images/test0244_22_W.png\n",
      "./test-images/test0245_16_Q.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0246_14_O.png\n",
      "./test-images/test0247_25_Z.png\n",
      "./test-images/test0248_7_H.png\n",
      "./test-images/test0249_13_N.png\n",
      "./test-images/test0250_15_P.png\n",
      "./test-images/test0251_6_G.png\n",
      "./test-images/test0252_1_B.png\n",
      "./test-images/test0253_2_C.png\n",
      "./test-images/test0254_18_S.png\n",
      "./test-images/test0255_9_J.png\n",
      "./test-images/test0256_28_space.png\n",
      "./test-images/test0257_20_U.png\n",
      "./test-images/test0258_9_J.png\n",
      "./test-images/test0259_2_C.png\n",
      "./test-images/test0260_19_T.png\n",
      "./test-images/test0261_22_W.png\n",
      "./test-images/test0262_28_space.png\n",
      "./test-images/test0263_2_C.png\n",
      "./test-images/test0264_27_nothing.png\n",
      "./test-images/test0265_3_D.png\n",
      "./test-images/test0266_20_U.png\n",
      "./test-images/test0267_4_E.png\n",
      "./test-images/test0268_20_U.png\n",
      "./test-images/test0269_5_F.png\n",
      "./test-images/test0270_18_S.png\n",
      "./test-images/test0271_8_I.png\n",
      "./test-images/test0272_24_Y.png\n",
      "./test-images/test0273_27_nothing.png\n",
      "./test-images/test0274_25_Z.png\n",
      "./test-images/test0275_19_T.png\n",
      "./test-images/test0276_2_C.png\n",
      "./test-images/test0277_5_F.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0278_27_nothing.png\n",
      "./test-images/test0279_9_J.png\n",
      "./test-images/test0280_8_I.png\n",
      "./test-images/test0281_24_Y.png\n",
      "./test-images/test0282_12_M.png\n",
      "./test-images/test0283_15_P.png\n",
      "./test-images/test0284_27_nothing.png\n",
      "./test-images/test0285_7_H.png\n",
      "./test-images/test0286_27_nothing.png\n",
      "./test-images/test0287_4_E.png\n",
      "./test-images/test0288_3_D.png\n",
      "./test-images/test0289_6_G.png\n",
      "./test-images/test0290_25_Z.png\n",
      "./test-images/test0291_0_A.png\n",
      "./test-images/test0292_3_D.png\n",
      "./test-images/test0293_23_X.png\n",
      "./test-images/test0294_21_V.png\n",
      "./test-images/test0295_15_P.png\n",
      "./test-images/test0296_20_U.png\n",
      "./test-images/test0297_22_W.png\n",
      "./test-images/test0298_28_space.png\n",
      "./test-images/test0299_27_nothing.png\n",
      "./test-images/test0300_7_H.png\n",
      "./test-images/test0301_19_T.png\n",
      "./test-images/test0302_22_W.png\n",
      "./test-images/test0303_22_W.png\n",
      "./test-images/test0304_11_L.png\n",
      "./test-images/test0305_1_B.png\n",
      "./test-images/test0306_3_D.png\n",
      "./test-images/test0307_8_I.png\n",
      "./test-images/test0308_27_nothing.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0309_12_M.png\n",
      "./test-images/test0310_3_D.png\n",
      "./test-images/test0311_3_D.png\n",
      "./test-images/test0312_10_K.png\n",
      "./test-images/test0313_28_space.png\n",
      "./test-images/test0314_23_X.png\n",
      "./test-images/test0315_10_K.png\n",
      "./test-images/test0316_3_D.png\n",
      "./test-images/test0317_4_E.png\n",
      "./test-images/test0318_0_A.png\n",
      "./test-images/test0319_7_H.png\n",
      "./test-images/test0320_11_L.png\n",
      "./test-images/test0321_0_A.png\n",
      "./test-images/test0322_12_M.png\n",
      "./test-images/test0323_15_P.png\n",
      "./test-images/test0324_1_B.png\n",
      "./test-images/test0325_10_K.png\n",
      "./test-images/test0326_24_Y.png\n",
      "./test-images/test0327_17_R.png\n",
      "./test-images/test0328_26_del.png\n",
      "./test-images/test0329_21_V.png\n",
      "./test-images/test0330_4_E.png\n",
      "./test-images/test0331_14_O.png\n",
      "./test-images/test0332_20_U.png\n",
      "./test-images/test0333_19_T.png\n",
      "./test-images/test0334_10_K.png\n",
      "./test-images/test0335_18_S.png\n",
      "./test-images/test0336_14_O.png\n",
      "./test-images/test0337_20_U.png\n",
      "./test-images/test0338_28_space.png\n",
      "./test-images/test0339_22_W.png\n",
      "./test-images/test0340_17_R.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0341_16_Q.png\n",
      "./test-images/test0342_7_H.png\n",
      "./test-images/test0343_0_A.png\n",
      "./test-images/test0344_25_Z.png\n",
      "./test-images/test0345_23_X.png\n",
      "./test-images/test0346_16_Q.png\n",
      "./test-images/test0347_26_del.png\n",
      "./test-images/test0348_10_K.png\n",
      "./test-images/test0349_21_V.png\n",
      "./test-images/test0350_14_O.png\n",
      "./test-images/test0351_12_M.png\n",
      "./test-images/test0352_18_S.png\n",
      "./test-images/test0353_22_W.png\n",
      "./test-images/test0354_11_L.png\n",
      "./test-images/test0355_19_T.png\n",
      "./test-images/test0356_14_O.png\n",
      "./test-images/test0357_5_F.png\n",
      "./test-images/test0358_26_del.png\n",
      "./test-images/test0359_5_F.png\n",
      "./test-images/test0360_4_E.png\n",
      "./test-images/test0361_7_H.png\n",
      "./test-images/test0362_25_Z.png\n",
      "./test-images/test0363_18_S.png\n",
      "./test-images/test0364_14_O.png\n",
      "./test-images/test0365_7_H.png\n",
      "./test-images/test0366_0_A.png\n",
      "./test-images/test0367_15_P.png\n",
      "./test-images/test0368_12_M.png\n",
      "./test-images/test0369_11_L.png\n",
      "./test-images/test0370_18_S.png\n",
      "./test-images/test0371_15_P.png\n",
      "./test-images/test0372_2_C.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0373_7_H.png\n",
      "./test-images/test0374_7_H.png\n",
      "./test-images/test0375_15_P.png\n",
      "./test-images/test0376_27_nothing.png\n",
      "./test-images/test0377_22_W.png\n",
      "./test-images/test0378_26_del.png\n",
      "./test-images/test0379_1_B.png\n",
      "./test-images/test0380_4_E.png\n",
      "./test-images/test0381_20_U.png\n",
      "./test-images/test0382_5_F.png\n",
      "./test-images/test0383_6_G.png\n",
      "./test-images/test0384_24_Y.png\n",
      "./test-images/test0385_7_H.png\n",
      "./test-images/test0386_24_Y.png\n",
      "./test-images/test0387_16_Q.png\n",
      "./test-images/test0388_16_Q.png\n",
      "./test-images/test0389_5_F.png\n",
      "./test-images/test0390_26_del.png\n",
      "./test-images/test0391_26_del.png\n",
      "./test-images/test0392_4_E.png\n",
      "./test-images/test0393_15_P.png\n",
      "./test-images/test0394_11_L.png\n",
      "./test-images/test0395_12_M.png\n",
      "./test-images/test0396_27_nothing.png\n",
      "./test-images/test0397_22_W.png\n",
      "./test-images/test0398_24_Y.png\n",
      "./test-images/test0399_8_I.png\n",
      "./test-images/test0400_6_G.png\n",
      "./test-images/test0401_11_L.png\n",
      "./test-images/test0402_27_nothing.png\n",
      "./test-images/test0403_17_R.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0404_3_D.png\n",
      "./test-images/test0405_26_del.png\n",
      "./test-images/test0406_23_X.png\n",
      "./test-images/test0407_6_G.png\n",
      "./test-images/test0408_25_Z.png\n",
      "./test-images/test0409_21_V.png\n",
      "./test-images/test0410_7_H.png\n",
      "./test-images/test0411_17_R.png\n",
      "./test-images/test0412_11_L.png\n",
      "./test-images/test0413_14_O.png\n",
      "./test-images/test0414_23_X.png\n",
      "./test-images/test0415_20_U.png\n",
      "./test-images/test0416_23_X.png\n",
      "./test-images/test0417_24_Y.png\n",
      "./test-images/test0418_6_G.png\n",
      "./test-images/test0419_6_G.png\n",
      "./test-images/test0420_28_space.png\n",
      "./test-images/test0421_18_S.png\n",
      "./test-images/test0422_24_Y.png\n",
      "./test-images/test0423_3_D.png\n",
      "./test-images/test0424_3_D.png\n",
      "./test-images/test0425_3_D.png\n",
      "./test-images/test0426_0_A.png\n",
      "./test-images/test0427_28_space.png\n",
      "./test-images/test0428_5_F.png\n",
      "./test-images/test0429_14_O.png\n",
      "./test-images/test0430_6_G.png\n",
      "./test-images/test0431_23_X.png\n",
      "./test-images/test0432_20_U.png\n",
      "./test-images/test0433_8_I.png\n",
      "./test-images/test0434_26_del.png\n",
      "./test-images/test0435_19_T.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0436_27_nothing.png\n",
      "./test-images/test0437_19_T.png\n",
      "./test-images/test0438_8_I.png\n",
      "./test-images/test0439_14_O.png\n",
      "./test-images/test0440_14_O.png\n",
      "./test-images/test0441_19_T.png\n",
      "./test-images/test0442_21_V.png\n",
      "./test-images/test0443_21_V.png\n",
      "./test-images/test0444_15_P.png\n",
      "./test-images/test0445_18_S.png\n",
      "./test-images/test0446_27_nothing.png\n",
      "./test-images/test0447_21_V.png\n",
      "./test-images/test0448_2_C.png\n",
      "./test-images/test0449_9_J.png\n",
      "./test-images/test0450_4_E.png\n",
      "./test-images/test0451_23_X.png\n",
      "./test-images/test0452_18_S.png\n",
      "./test-images/test0453_1_B.png\n",
      "./test-images/test0454_24_Y.png\n",
      "./test-images/test0455_12_M.png\n",
      "./test-images/test0456_12_M.png\n",
      "./test-images/test0457_28_space.png\n",
      "./test-images/test0458_8_I.png\n",
      "./test-images/test0459_21_V.png\n",
      "./test-images/test0460_19_T.png\n",
      "./test-images/test0461_4_E.png\n",
      "./test-images/test0462_18_S.png\n",
      "./test-images/test0463_8_I.png\n",
      "./test-images/test0464_4_E.png\n",
      "./test-images/test0465_13_N.png\n",
      "./test-images/test0466_6_G.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0467_8_I.png\n",
      "./test-images/test0468_5_F.png\n",
      "./test-images/test0469_18_S.png\n",
      "./test-images/test0470_26_del.png\n",
      "./test-images/test0471_25_Z.png\n",
      "./test-images/test0472_2_C.png\n",
      "./test-images/test0473_11_L.png\n",
      "./test-images/test0474_15_P.png\n",
      "./test-images/test0475_12_M.png\n",
      "./test-images/test0476_16_Q.png\n",
      "./test-images/test0477_12_M.png\n",
      "./test-images/test0478_2_C.png\n",
      "./test-images/test0479_2_C.png\n",
      "./test-images/test0480_13_N.png\n",
      "./test-images/test0481_2_C.png\n",
      "./test-images/test0482_25_Z.png\n",
      "./test-images/test0483_2_C.png\n",
      "./test-images/test0484_24_Y.png\n",
      "./test-images/test0485_18_S.png\n",
      "./test-images/test0486_22_W.png\n",
      "./test-images/test0487_12_M.png\n",
      "./test-images/test0488_5_F.png\n",
      "./test-images/test0489_22_W.png\n",
      "./test-images/test0490_13_N.png\n",
      "./test-images/test0491_25_Z.png\n",
      "./test-images/test0492_13_N.png\n",
      "./test-images/test0493_5_F.png\n",
      "./test-images/test0494_28_space.png\n",
      "./test-images/test0495_26_del.png\n",
      "./test-images/test0496_11_L.png\n",
      "./test-images/test0497_19_T.png\n",
      "./test-images/test0498_8_I.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0499_26_del.png\n",
      "./test-images/test0500_8_I.png\n",
      "./test-images/test0501_28_space.png\n",
      "./test-images/test0502_1_B.png\n",
      "./test-images/test0503_23_X.png\n",
      "./test-images/test0504_15_P.png\n",
      "./test-images/test0505_14_O.png\n",
      "./test-images/test0506_11_L.png\n",
      "./test-images/test0507_22_W.png\n",
      "./test-images/test0508_8_I.png\n",
      "./test-images/test0509_22_W.png\n",
      "./test-images/test0510_21_V.png\n",
      "./test-images/test0511_5_F.png\n",
      "./test-images/test0512_25_Z.png\n",
      "./test-images/test0513_0_A.png\n",
      "./test-images/test0514_20_U.png\n",
      "./test-images/test0515_20_U.png\n",
      "./test-images/test0516_1_B.png\n",
      "./test-images/test0517_12_M.png\n",
      "./test-images/test0518_11_L.png\n",
      "./test-images/test0519_21_V.png\n",
      "./test-images/test0520_4_E.png\n",
      "./test-images/test0521_20_U.png\n",
      "./test-images/test0522_15_P.png\n",
      "./test-images/test0523_18_S.png\n",
      "./test-images/test0524_19_T.png\n",
      "./test-images/test0525_14_O.png\n",
      "./test-images/test0526_19_T.png\n",
      "./test-images/test0527_28_space.png\n",
      "./test-images/test0528_17_R.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0529_3_D.png\n",
      "./test-images/test0530_0_A.png\n",
      "./test-images/test0531_5_F.png\n",
      "./test-images/test0532_26_del.png\n",
      "./test-images/test0533_13_N.png\n",
      "./test-images/test0534_13_N.png\n",
      "./test-images/test0535_20_U.png\n",
      "./test-images/test0536_9_J.png\n",
      "./test-images/test0537_22_W.png\n",
      "./test-images/test0538_19_T.png\n",
      "./test-images/test0539_21_V.png\n",
      "./test-images/test0540_10_K.png\n",
      "./test-images/test0541_3_D.png\n",
      "./test-images/test0542_11_L.png\n",
      "./test-images/test0543_16_Q.png\n",
      "./test-images/test0544_19_T.png\n",
      "./test-images/test0545_0_A.png\n",
      "./test-images/test0546_3_D.png\n",
      "./test-images/test0547_5_F.png\n",
      "./test-images/test0548_20_U.png\n",
      "./test-images/test0549_16_Q.png\n",
      "./test-images/test0550_14_O.png\n",
      "./test-images/test0551_25_Z.png\n",
      "./test-images/test0552_21_V.png\n",
      "./test-images/test0553_28_space.png\n",
      "./test-images/test0554_6_G.png\n",
      "./test-images/test0555_12_M.png\n",
      "./test-images/test0556_27_nothing.png\n",
      "./test-images/test0557_21_V.png\n",
      "./test-images/test0558_6_G.png\n",
      "./test-images/test0559_7_H.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0560_26_del.png\n",
      "./test-images/test0561_14_O.png\n",
      "./test-images/test0562_10_K.png\n",
      "./test-images/test0563_6_G.png\n",
      "./test-images/test0564_4_E.png\n",
      "./test-images/test0565_21_V.png\n",
      "./test-images/test0566_21_V.png\n",
      "./test-images/test0567_22_W.png\n",
      "./test-images/test0568_16_Q.png\n",
      "./test-images/test0569_18_S.png\n",
      "./test-images/test0570_23_X.png\n",
      "./test-images/test0571_28_space.png\n",
      "./test-images/test0572_18_S.png\n",
      "./test-images/test0573_0_A.png\n",
      "./test-images/test0574_26_del.png\n",
      "./test-images/test0575_13_N.png\n",
      "./test-images/test0576_23_X.png\n",
      "./test-images/test0577_23_X.png\n",
      "./test-images/test0578_0_A.png\n",
      "./test-images/test0579_4_E.png\n",
      "./test-images/test0580_18_S.png\n",
      "./test-images/test0581_6_G.png\n",
      "./test-images/test0582_28_space.png\n",
      "./test-images/test0583_23_X.png\n",
      "./test-images/test0584_7_H.png\n",
      "./test-images/test0585_18_S.png\n",
      "./test-images/test0586_21_V.png\n",
      "./test-images/test0587_1_B.png\n",
      "./test-images/test0588_1_B.png\n",
      "./test-images/test0589_13_N.png\n",
      "./test-images/test0590_11_L.png\n",
      "./test-images/test0591_6_G.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0592_20_U.png\n",
      "./test-images/test0593_24_Y.png\n",
      "./test-images/test0594_10_K.png\n",
      "./test-images/test0595_16_Q.png\n",
      "./test-images/test0596_8_I.png\n",
      "./test-images/test0597_17_R.png\n",
      "./test-images/test0598_7_H.png\n",
      "./test-images/test0599_16_Q.png\n",
      "./test-images/test0600_17_R.png\n",
      "./test-images/test0601_1_B.png\n",
      "./test-images/test0602_19_T.png\n",
      "./test-images/test0603_18_S.png\n",
      "./test-images/test0604_19_T.png\n",
      "./test-images/test0605_22_W.png\n",
      "./test-images/test0606_2_C.png\n",
      "./test-images/test0607_27_nothing.png\n",
      "./test-images/test0608_12_M.png\n",
      "./test-images/test0609_19_T.png\n",
      "./test-images/test0610_11_L.png\n",
      "./test-images/test0611_3_D.png\n",
      "./test-images/test0612_5_F.png\n",
      "./test-images/test0613_13_N.png\n",
      "./test-images/test0614_20_U.png\n",
      "./test-images/test0615_10_K.png\n",
      "./test-images/test0616_24_Y.png\n",
      "./test-images/test0617_11_L.png\n",
      "./test-images/test0618_12_M.png\n",
      "./test-images/test0619_9_J.png\n",
      "./test-images/test0620_4_E.png\n",
      "./test-images/test0621_4_E.png\n",
      "./test-images/test0622_2_C.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0623_8_I.png\n",
      "./test-images/test0624_17_R.png\n",
      "./test-images/test0625_7_H.png\n",
      "./test-images/test0626_24_Y.png\n",
      "./test-images/test0627_0_A.png\n",
      "./test-images/test0628_17_R.png\n",
      "./test-images/test0629_5_F.png\n",
      "./test-images/test0630_6_G.png\n",
      "./test-images/test0631_20_U.png\n",
      "./test-images/test0632_10_K.png\n",
      "./test-images/test0633_15_P.png\n",
      "./test-images/test0634_10_K.png\n",
      "./test-images/test0635_23_X.png\n",
      "./test-images/test0636_24_Y.png\n",
      "./test-images/test0637_25_Z.png\n",
      "./test-images/test0638_14_O.png\n",
      "./test-images/test0639_27_nothing.png\n",
      "./test-images/test0640_3_D.png\n",
      "./test-images/test0641_3_D.png\n",
      "./test-images/test0642_5_F.png\n",
      "./test-images/test0643_20_U.png\n",
      "./test-images/test0644_2_C.png\n",
      "./test-images/test0645_21_V.png\n",
      "./test-images/test0646_5_F.png\n",
      "./test-images/test0647_19_T.png\n",
      "./test-images/test0648_16_Q.png\n",
      "./test-images/test0649_16_Q.png\n",
      "./test-images/test0650_24_Y.png\n",
      "./test-images/test0651_12_M.png\n",
      "./test-images/test0652_20_U.png\n",
      "./test-images/test0653_1_B.png\n",
      "./test-images/test0654_14_O.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0655_4_E.png\n",
      "./test-images/test0656_13_N.png\n",
      "./test-images/test0657_15_P.png\n",
      "./test-images/test0658_26_del.png\n",
      "./test-images/test0659_10_K.png\n",
      "./test-images/test0660_23_X.png\n",
      "./test-images/test0661_6_G.png\n",
      "./test-images/test0662_20_U.png\n",
      "./test-images/test0663_0_A.png\n",
      "./test-images/test0664_28_space.png\n",
      "./test-images/test0665_15_P.png\n",
      "./test-images/test0666_28_space.png\n",
      "./test-images/test0667_8_I.png\n",
      "./test-images/test0668_23_X.png\n",
      "./test-images/test0669_0_A.png\n",
      "./test-images/test0670_25_Z.png\n",
      "./test-images/test0671_15_P.png\n",
      "./test-images/test0672_22_W.png\n",
      "./test-images/test0673_4_E.png\n",
      "./test-images/test0674_25_Z.png\n",
      "./test-images/test0675_18_S.png\n",
      "./test-images/test0676_10_K.png\n",
      "./test-images/test0677_3_D.png\n",
      "./test-images/test0678_20_U.png\n",
      "./test-images/test0679_13_N.png\n",
      "./test-images/test0680_12_M.png\n",
      "./test-images/test0681_18_S.png\n",
      "./test-images/test0682_7_H.png\n",
      "./test-images/test0683_24_Y.png\n",
      "./test-images/test0684_11_L.png\n",
      "./test-images/test0685_17_R.png\n",
      "./test-images/test0686_9_J.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0687_27_nothing.png\n",
      "./test-images/test0688_13_N.png\n",
      "./test-images/test0689_2_C.png\n",
      "./test-images/test0690_1_B.png\n",
      "./test-images/test0691_7_H.png\n",
      "./test-images/test0692_12_M.png\n",
      "./test-images/test0693_26_del.png\n",
      "./test-images/test0694_28_space.png\n",
      "./test-images/test0695_13_N.png\n",
      "./test-images/test0696_3_D.png\n",
      "./test-images/test0697_10_K.png\n",
      "./test-images/test0698_19_T.png\n",
      "./test-images/test0699_22_W.png\n",
      "./test-images/test0700_10_K.png\n",
      "./test-images/test0701_25_Z.png\n",
      "./test-images/test0702_17_R.png\n",
      "./test-images/test0703_26_del.png\n",
      "./test-images/test0704_5_F.png\n",
      "./test-images/test0705_5_F.png\n",
      "./test-images/test0706_10_K.png\n",
      "./test-images/test0707_3_D.png\n",
      "./test-images/test0708_26_del.png\n",
      "./test-images/test0709_8_I.png\n",
      "./test-images/test0710_8_I.png\n",
      "./test-images/test0711_19_T.png\n",
      "./test-images/test0712_12_M.png\n",
      "./test-images/test0713_12_M.png\n",
      "./test-images/test0714_27_nothing.png\n",
      "./test-images/test0715_21_V.png\n",
      "./test-images/test0716_9_J.png\n",
      "./test-images/test0717_11_L.png\n",
      "./test-images/test0718_7_H.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0719_1_B.png\n",
      "./test-images/test0720_15_P.png\n",
      "./test-images/test0721_27_nothing.png\n",
      "./test-images/test0722_9_J.png\n",
      "./test-images/test0723_3_D.png\n",
      "./test-images/test0724_16_Q.png\n",
      "./test-images/test0725_13_N.png\n",
      "./test-images/test0726_24_Y.png\n",
      "./test-images/test0727_1_B.png\n",
      "./test-images/test0728_4_E.png\n",
      "./test-images/test0729_7_H.png\n",
      "./test-images/test0730_6_G.png\n",
      "./test-images/test0731_8_I.png\n",
      "./test-images/test0732_25_Z.png\n",
      "./test-images/test0733_22_W.png\n",
      "./test-images/test0734_23_X.png\n",
      "./test-images/test0735_7_H.png\n",
      "./test-images/test0736_0_A.png\n",
      "./test-images/test0737_23_X.png\n",
      "./test-images/test0738_20_U.png\n",
      "./test-images/test0739_19_T.png\n",
      "./test-images/test0740_25_Z.png\n",
      "./test-images/test0741_17_R.png\n",
      "./test-images/test0742_25_Z.png\n",
      "./test-images/test0743_13_N.png\n",
      "./test-images/test0744_3_D.png\n",
      "./test-images/test0745_10_K.png\n",
      "./test-images/test0746_9_J.png\n",
      "./test-images/test0747_1_B.png\n",
      "./test-images/test0748_5_F.png\n",
      "./test-images/test0749_5_F.png\n",
      "./test-images/test0750_11_L.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0751_18_S.png\n",
      "./test-images/test0752_19_T.png\n",
      "./test-images/test0753_6_G.png\n",
      "./test-images/test0754_28_space.png\n",
      "./test-images/test0755_11_L.png\n",
      "./test-images/test0756_21_V.png\n",
      "./test-images/test0757_22_W.png\n",
      "./test-images/test0758_13_N.png\n",
      "./test-images/test0759_25_Z.png\n",
      "./test-images/test0760_5_F.png\n",
      "./test-images/test0761_0_A.png\n",
      "./test-images/test0762_2_C.png\n",
      "./test-images/test0763_12_M.png\n",
      "./test-images/test0764_26_del.png\n",
      "./test-images/test0765_17_R.png\n",
      "./test-images/test0766_1_B.png\n",
      "./test-images/test0767_12_M.png\n",
      "./test-images/test0768_15_P.png\n",
      "./test-images/test0769_25_Z.png\n",
      "./test-images/test0770_9_J.png\n",
      "./test-images/test0771_10_K.png\n",
      "./test-images/test0772_0_A.png\n",
      "./test-images/test0773_17_R.png\n",
      "./test-images/test0774_0_A.png\n",
      "./test-images/test0775_5_F.png\n",
      "./test-images/test0776_27_nothing.png\n",
      "./test-images/test0777_26_del.png\n",
      "./test-images/test0778_27_nothing.png\n",
      "./test-images/test0779_1_B.png\n",
      "./test-images/test0780_1_B.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0781_17_R.png\n",
      "./test-images/test0782_16_Q.png\n",
      "./test-images/test0783_11_L.png\n",
      "./test-images/test0784_5_F.png\n",
      "./test-images/test0785_22_W.png\n",
      "./test-images/test0786_18_S.png\n",
      "./test-images/test0787_0_A.png\n",
      "./test-images/test0788_21_V.png\n",
      "./test-images/test0789_25_Z.png\n",
      "./test-images/test0790_1_B.png\n",
      "./test-images/test0791_22_W.png\n",
      "./test-images/test0792_3_D.png\n",
      "./test-images/test0793_27_nothing.png\n",
      "./test-images/test0794_5_F.png\n",
      "./test-images/test0795_11_L.png\n",
      "./test-images/test0796_3_D.png\n",
      "./test-images/test0797_20_U.png\n",
      "./test-images/test0798_16_Q.png\n",
      "./test-images/test0799_7_H.png\n",
      "./test-images/test0800_19_T.png\n",
      "./test-images/test0801_22_W.png\n",
      "./test-images/test0802_18_S.png\n",
      "./test-images/test0803_7_H.png\n",
      "./test-images/test0804_22_W.png\n",
      "./test-images/test0805_28_space.png\n",
      "./test-images/test0806_6_G.png\n",
      "./test-images/test0807_25_Z.png\n",
      "./test-images/test0808_4_E.png\n",
      "./test-images/test0809_5_F.png\n",
      "./test-images/test0810_7_H.png\n",
      "./test-images/test0811_21_V.png\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "./test-images/test0812_4_E.png\n",
      "./test-images/test0813_1_B.png\n",
      "./test-images/test0814_14_O.png\n",
      "./test-images/test0815_5_F.png\n",
      "./test-images/test0816_18_S.png\n",
      "./test-images/test0817_22_W.png\n",
      "./test-images/test0818_22_W.png\n",
      "./test-images/test0819_16_Q.png\n",
      "./test-images/test0820_22_W.png\n",
      "./test-images/test0821_18_S.png\n",
      "./test-images/test0822_26_del.png\n",
      "./test-images/test0823_8_I.png\n",
      "./test-images/test0824_14_O.png\n",
      "./test-images/test0825_8_I.png\n",
      "./test-images/test0826_17_R.png\n",
      "./test-images/test0827_13_N.png\n",
      "./test-images/test0828_22_W.png\n",
      "./test-images/test0829_21_V.png\n",
      "./test-images/test0830_25_Z.png\n",
      "./test-images/test0831_9_J.png\n",
      "./test-images/test0832_10_K.png\n",
      "./test-images/test0833_27_nothing.png\n",
      "./test-images/test0834_24_Y.png\n",
      "./test-images/test0835_12_M.png\n",
      "./test-images/test0836_27_nothing.png\n",
      "./test-images/test0837_13_N.png\n",
      "./test-images/test0838_0_A.png\n",
      "./test-images/test0839_1_B.png\n",
      "./test-images/test0840_5_F.png\n",
      "./test-images/test0841_7_H.png\n",
      "./test-images/test0842_7_H.png\n",
      "./test-images/test0843_22_W.png\n",
      "1/1 [==============================] - 1s 677ms/step\n",
      "./test-images/test0844_4_E.png\n",
      "./test-images/test0845_17_R.png\n",
      "./test-images/test0846_13_N.png\n",
      "./test-images/test0847_0_A.png\n",
      "./test-images/test0848_8_I.png\n",
      "./test-images/test0849_1_B.png\n"
     ]
    }
   ],
   "source": [
    "generate_test_images(valid_dataset, TrainingConfig.CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78e2b5",
   "metadata": {},
   "source": [
    "## 6 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3256950",
   "metadata": {},
   "source": [
    "In this notebook, we showed how to quantize and compile a TensorFlow2 model with Vitis-AI for deployment on AMD Zynq-UltraScale+ devices. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "c4_03_15a_ASL150_VGG_TransferLearn_FineTune_Conv2D_Train_Dense.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
